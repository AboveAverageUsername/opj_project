1
00:00:06,000 --> 00:00:12,074
Оглашавајте свој производ или марку код нас; контактирајте www.OpenSubtitles.org

2
00:00:30,000 --> 00:00:33,500
Prevod i obrada titla:
politfilm.com

3
00:00:35,078 --> 00:00:41,302
Na rubu smo sveta
sve intenzivnije,

4
00:00:41,345 --> 00:00:45,219
sofisticiranije veštačke inteligencije.

5
00:00:45,262 --> 00:00:48,396
Tehnologija se razvija
znatno brže od našeg društva

6
00:00:48,439 --> 00:00:51,181
i ima mogućnost
da zaštiti nas kao građane.

7
00:00:51,486 --> 00:00:55,707
Roboti dolaze,
i oni će uništiti naše živote.

8
00:01:01,887 --> 00:01:04,238
Postoji umrežena inteligencija
koja nas posmatra,

9
00:01:04,281 --> 00:01:08,590
zna sve o nama,
i počinje pokušaje da nas promeni.

10
00:01:08,633 --> 00:01:12,768
Tviter je postao
vodeći svetski medijski sajt.

11
00:01:12,811 --> 00:01:15,205
Tehnologija nije nikad dobra ili loša.

12
00:01:15,249 --> 00:01:18,948
Radi se o tome šta mi radimo
sa tom tehnologijom.

13
00:01:18,991 --> 00:01:22,734
U jednom momentu milioni ljudi
će biti otpušteni sa poslova

14
00:01:22,778 --> 00:01:25,737
jer će njihove veštine
postati suvišne.

15
00:01:25,781 --> 00:01:27,435
Masovna nezaposlenost...

16
00:01:27,478 --> 00:01:31,727
povećanje nejednakosti,
čak i društveni nemiri.

17
00:01:32,570 --> 00:01:35,530
Bez obzra na to
da li se treba plašiti ili ne,

18
00:01:35,573 --> 00:01:38,185
promena dolazi,
i niko je ne može zaustaviti.

19
00:01:44,582 --> 00:01:48,146
Uložili smo ogromne količine novca,
tako da je logično pretpostaviti

20
00:01:48,148 --> 00:01:50,893
da će vojska,
u skladu sa svojim ciljevima,

21
00:01:50,936 --> 00:01:53,330
početi da koristi
ove tehnologije.

22
00:01:53,374 --> 00:01:57,552
Sistemi autonomih oružja
mogu dovesti do globalne trke u naoružanju

23
00:01:57,595 --> 00:02:00,032
koja će biti slična ili veća od one 
u vreme eksanzije nuklearnog naoružanja.

24
00:02:02,339 --> 00:02:05,429
Znamo već koji je odgovor.
Oni će nas na kraju ubijati.

25
00:02:10,826 --> 00:02:15,874
Ovi tehnološki skokovi
doneće neverovatna čuda

26
00:02:15,918 --> 00:02:18,181
i neverovatne užase.

27
00:02:24,274 --> 00:02:29,323
Mi smo je stvorili, tako da, 
ja mislim, kako budemo napredovali,

28
00:02:29,366 --> 00:02:33,762
ova inteligencija će
sadržati delove nas.

29
00:02:33,805 --> 00:02:35,981
Tako da mislim da je pitanje:

30
00:02:36,025 --> 00:02:39,463
Hoće li sadržati
one pozitivne delove...

31
00:02:39,507 --> 00:02:41,378
ili one loše?

32
00:03:04,836 --> 00:03:08,840
Preživeli su
nazvali ovaj rat "Sudnji dan".

33
00:03:08,884 --> 00:03:12,583
I dočekali su novi košmar:

34
00:03:12,627 --> 00:03:14,319
Rat protiv mašina.

35
00:03:15,456 --> 00:03:18,023
Ja mislim da smo
ovo totalno sjebali.

36
00:03:18,067 --> 00:03:21,549
Mislim da je javno mnjenje
posredstvom Holivuda postalo imuno

37
00:03:21,592 --> 00:03:24,247
na ovaj problem.

38
00:03:24,291 --> 00:03:28,251
Ideja da će mašine
preuzeti svet.

39
00:03:28,295 --> 00:03:30,645
Otvori vrata doka, HAL.

40
00:03:30,688 --> 00:03:33,561
Žao mi je Dejv.

41
00:03:33,604 --> 00:03:35,911
Bojim se da to ne mogu učiniti.

42
00:03:37,434 --> 00:03:38,696
HAL?

43
00:03:38,740 --> 00:03:40,437
Dovoljno puta smo vikali Vuk! ...

44
00:03:40,481 --> 00:03:42,483
... da je na kraju javnost
prestala da obraća pažnju,

45
00:03:42,484 --> 00:03:44,120
jer se svi osećaju kao da je to
naučna fantastika.

46
00:03:44,121 --> 00:03:46,001
Samo ovo što sedim ovde
i pričam o tome sada,

47
00:03:46,002 --> 00:03:48,301
pomalo odaje utisak šašavog,
nešto kao,

48
00:03:48,302 --> 00:03:51,697
"O, je li to artefakt
nekog filma iz B-produkcije?"

49
00:03:51,709 --> 00:03:56,584
WOPR je sve vreme zaposlen
na predviđanju III svetskog rata.

50
00:03:56,627 --> 00:03:59,064
Ali zapravo nije.

51
00:03:59,108 --> 00:04:02,111
Javno mnjenje verovatno neće 
dobiti dovoljno informacija o ovome.

52
00:04:11,555 --> 00:04:13,514
I kao društvo i kao pojedinci,

53
00:04:13,557 --> 00:04:18,954
sve više smo okruženi
inteligentnim mašinama.

54
00:04:18,997 --> 00:04:22,653
Nisamo ovaj džepni uređaj
na dlanu ruke

55
00:04:22,697 --> 00:04:24,829
i koristimo ga kako bismo
doneli mnoštvo povezanih

56
00:04:24,873 --> 00:04:26,831
životnih odluka upravo sada,

57
00:04:26,875 --> 00:04:29,007
a sve uz pomoć algoritama

58
00:04:29,051 --> 00:04:30,748
kojih nismo ni svesni.

59
00:04:34,186 --> 00:04:36,537
Već smo prilično 
zadovoljeni idejom

60
00:04:36,580 --> 00:04:37,929
da možemo razgovarati sa svojim telefonom,

61
00:04:37,973 --> 00:04:40,062
i da nas on uglavnom razume.

62
00:04:40,105 --> 00:04:42,456
Pronašla sam veliki broj 
akcionih filmova.

63
00:04:42,499 --> 00:04:44,327
Pre pet godina -- nije bilo šanse.

64
00:04:44,371 --> 00:04:47,678
Roboti.
Mašine koje vide i govore...

65
00:04:47,722 --> 00:04:48,897
...i koje slušaju.

66
00:04:48,940 --> 00:04:50,202
Sve je to sada stvarnost.

67
00:04:50,246 --> 00:04:51,639
Sve ove tehnologije

68
00:04:51,682 --> 00:04:54,886
će iz korena promeniti
naše društvo.

69
00:04:55,730 --> 00:05:00,212
Trenutno imamo ovaj veliki
promet samovozećih vozila.

70
00:05:00,256 --> 00:05:01,953
Autonomna vožnja automobila

71
00:05:01,997 --> 00:05:05,688
može promeniti živote
ljudi na bolje.

72
00:05:06,131 --> 00:05:09,570
Brojne članove familije,
uključujući moju majku,

73
00:05:09,613 --> 00:05:11,876
mog brata i snaju
i njihovu decu,

74
00:05:11,920 --> 00:05:14,009
sam izgubio u saobraćajnim nesrećama.

75
00:05:14,052 --> 00:05:18,405
Vrlo je očito da skoro
možemo da eliminišemo automobilske udese.

76
00:05:18,448 --> 00:05:20,102
korišćenjem automatizacije.

77
00:05:20,145 --> 00:05:21,843
30,000 života godišnje 
i to samo u SAD.

78
00:05:21,886 --> 00:05:24,955
Oko milion ljudi širom sveta
godišnje.

79
00:05:25,499 --> 00:05:27,501
U zdravstvu, rani pokazatelji

80
00:05:27,544 --> 00:05:29,503
su zapravo najvažniji
u toj oblasti,

81
00:05:29,546 --> 00:05:33,158
tako da je to još jedno mesto
gde ona može spasiti nečiji život.

82
00:05:33,202 --> 00:05:35,726
Ovde, u centru za
lečenje raka dojke,

83
00:05:35,770 --> 00:05:38,381
sve one stvari koje
mozak radiologa

84
00:05:38,425 --> 00:05:43,386
uradi za dva minuta,
kompjuter završi istog trenutka.

85
00:05:43,430 --> 00:05:47,303
Kompjuter je pogledao
1 milijardu mamograma,

86
00:05:47,347 --> 00:05:49,261
i on uzima te podatke
i upoređuje ih odmah

87
00:05:49,305 --> 00:05:51,438
sa ovom slikom,

88
00:05:51,481 --> 00:05:54,441
tako da je korišćenje u
medicini od velikog značaja.

89
00:05:56,399 --> 00:05:59,402
Još jedna oblast
u kojoj predviđamo veliki razvoj

90
00:05:59,446 --> 00:06:03,275
je zapravo razumevanje
našeg genetskog koda

91
00:06:03,319 --> 00:06:06,104
i korišćenje tog znanja
i u davanju dijagnoza

92
00:06:06,148 --> 00:06:07,758
i u kreiranju
personalizovanih tretmana.

93
00:06:11,675 --> 00:06:14,112
Primarna namena
svih ovih mašina

94
00:06:14,156 --> 00:06:17,246
biće usmerena na poboljšanje
naše sopstvene inteligencije.

95
00:06:17,289 --> 00:06:19,422
Bićemo u mogućnosti da
sebe načinimo pametnijima,

96
00:06:19,466 --> 00:06:22,543
i bićemo bolji 
u rešavanju problema.

97
00:06:22,586 --> 00:06:25,075
Ne moramo da starimo.
Jer ćemo zapravo razumeti starenje.

98
00:06:25,076 --> 00:06:27,126
I moći ćemo da zaustavimo taj proces.

99
00:06:27,169 --> 00:06:29,519
Tako da zaista ne postoji granica
u čemu sve inteligentne mašine

100
00:06:29,563 --> 00:06:30,868
mogu da pomognu ljudskoj rasi.

101
00:06:36,308 --> 00:06:39,399
Zar nije da pametnija mašina
uvek i bolja mašina?

102
00:06:42,053 --> 00:06:44,708
Teško je reći kada
sam tačno počeo da mislim

103
00:06:44,752 --> 00:06:46,971
da je to pomalo naivno.

104
00:06:56,503 --> 00:07:00,898
Stuart Russell, on je praktično bog
u oblasti veštačke inteligencije.

105
00:07:00,942 --> 00:07:04,380
Napisao je knjigu koju skoro
svaki fakultet koristi.

106
00:07:04,424 --> 00:07:06,948
Ranije sam govorio kako je to
najprodavaniji AI bestseler.

107
00:07:06,991 --> 00:07:10,255
Sada samo kažem: 
"To je najčešće kraden PDF".

108
00:07:13,694 --> 00:07:17,306
Cilj veštačke inteligencije je
da učini kompjutere pametnijim,

109
00:07:17,349 --> 00:07:19,830
ali iz opšte tačke gledišta
javnog mnjenja,

110
00:07:19,874 --> 00:07:21,484
ono što se računa kao AI
je samo nešto

111
00:07:21,528 --> 00:07:23,268
što je iznenađujuće inteligentno

112
00:07:23,312 --> 00:07:25,488
u poređenju sa onim
što smo mislili da će

113
00:07:25,532 --> 00:07:28,004
kompjuteri biti u stanju da urade.

114
00:07:28,448 --> 00:07:33,801
Polje istraživanja AI-ja
je da u osnovi pokuša da simulira

115
00:07:33,844 --> 00:07:36,717
sve vrste ljudskih sposobnosti.

116
00:07:36,760 --> 00:07:38,719
Mi smo u AI eri.

117
00:07:38,762 --> 00:07:40,503
Silicijumska dolina
ima mogućnost da se fokusira

118
00:07:40,547 --> 00:07:42,462
na jednu svetlu, aktuelnu stvar.

119
00:07:42,505 --> 00:07:45,508
U prošloj deceniji to je bilo
povezivanju preko neta i društvenih mreža,

120
00:07:45,552 --> 00:07:48,119
a sada je sasvim jasno 
da se fokus promenio.

121
00:07:48,163 --> 00:07:50,557
Sve počinje sa mašinskim učenjem.

122
00:07:50,600 --> 00:07:54,343
Kada se osvrnemo unazad, moramo se zapitati:
šta je bila je bila prva veštačka inteligencija?

123
00:07:54,386 --> 00:07:57,389
Nije preterano seksi i nije ono što 
smo navikli da vidimo u filmovima,

124
00:07:57,433 --> 00:08:00,741
ali bićete u pravu ako kažete
da je Google stvorio,

125
00:08:00,784 --> 00:08:03,395
ne samo pretraživač,
već i svevidećeg.

126
00:08:03,439 --> 00:08:06,486
I način da ljudi pitaju
bilo šta im padne na pamet

127
00:08:06,529 --> 00:08:08,270
i da dobiju odgovor koji im je potreban.

128
00:08:08,313 --> 00:08:11,273
Većina ljudi nije svesno
da ono što Google radi

129
00:08:11,316 --> 00:08:13,710
je zapravo oblik
veštačke inteligencije.

130
00:08:13,754 --> 00:08:16,234
Oni se samo nakače,
ukucaju tamo nešto.

131
00:08:16,278 --> 00:08:18,323
Google im da odgovor.

132
00:08:18,367 --> 00:08:21,444
Svakom pretragom,
treniramo ga da bude bolji.

133
00:08:21,445 --> 00:08:24,108
Ponekad ukucavamo u pretragu,
i on nam da odgovor

134
00:08:24,109 --> 00:08:27,434
čak pre nego što i 
završimo sa kucanjem.

135
00:08:27,463 --> 00:08:29,944
Znaš npr, ko je
predsednik Kazahstana?

136
00:08:29,987 --> 00:08:31,685
I on ti samo kaže.

137
00:08:31,728 --> 00:08:34,818
Ne moraš da odeš na službeni nacionalni 
sajt Kazahstana da bi to saznao.

138
00:08:34,862 --> 00:08:37,081
Do nedavno to nije bilo moguće.

139
00:08:37,125 --> 00:08:39,475
Ovo je veštačka inteligencija.

140
00:08:39,519 --> 00:08:42,783
Za nekoliko godina kada budemo 
pokušali da razumemo, reći ćemo,

141
00:08:42,826 --> 00:08:44,567
"Kako smo to propustili?"

142
00:08:44,611 --> 00:08:48,484
To je jedna od najuočljivijih
kontradiktornosti sa kojima se suočavamo.

143
00:08:48,528 --> 00:08:52,053
I Google i Facebook, i slični,
sagradili su čitave biznise zasnovane

144
00:08:52,096 --> 00:08:54,185
na davanju besplatnih stvari 
nama kao društvu.

145
00:08:54,229 --> 00:08:56,013
Ali to je Faustovska trgovina.

146
00:08:56,057 --> 00:09:00,017
Oni nešto iz nas 
izvlače za uzvrat,

147
00:09:00,061 --> 00:09:01,628
ali mi ne znamo

148
00:09:01,671 --> 00:09:03,760
kakav kod se pokreće
sa druge strane i zašto.

149
00:09:03,804 --> 00:09:05,846
Nemamo pojma.

150
00:09:06,589 --> 00:09:08,591
To govori upravo
o problemu

151
00:09:08,635 --> 00:09:11,028
koliko mi zapravo treba
da verujemo ovim mašinama.

152
00:09:14,162 --> 00:09:18,166
Koristim kompjutere
bukvalno za sve.

153
00:09:18,209 --> 00:09:21,386
Toliko je mnogo
prednosti kompjutera danas,

154
00:09:21,430 --> 00:09:23,824
i postali suveliki deo naših života.

155
00:09:23,867 --> 00:09:26,174
Prosto je neverovatno
šta sve kompjuter može.

156
00:09:26,217 --> 00:09:29,090
I zapravo ga možeš
nositi u tašni.

157
00:09:29,133 --> 00:09:31,571
Mislim, koliko je to fenomenalno?

158
00:09:31,614 --> 00:09:35,052
Mislim da je veći deo tehnologije
namenjen da nam olakša stvari

159
00:09:35,096 --> 00:09:37,315
i učini ih jednostavnijima,

160
00:09:37,359 --> 00:09:40,362
tako da se nadamo da 
će to ostati cilj.

161
00:09:40,405 --> 00:09:43,147
Mislim da svi vole
svoje kompjutere.

162
00:09:51,721 --> 00:09:53,810
Ljudi ne shvataju
da su u konstantnim

163
00:09:53,854 --> 00:09:59,076
pregovorima
sa mašinama;

164
00:09:59,120 --> 00:10:02,993
bilo da se radi o cenama
proizvoda u vašoj korpi na Amazonu,

165
00:10:03,037 --> 00:10:05,517
bilo da li možete da
rezervišete određeni let,

166
00:10:05,561 --> 00:10:08,912
ili sobu u 
određenom hotelu.

167
00:10:08,956 --> 00:10:11,959
Ono što se tada dešava je
da su algoritmi mašinskog učenja

168
00:10:12,002 --> 00:10:14,265
odlučili da 
osoba kao vi

169
00:10:14,309 --> 00:10:17,791
će verovatno platiti 2 centa više
i u skladu s tim menja cenu.

170
00:10:21,795 --> 00:10:24,014
Stvar je u tome da kompjuter
posmatra milione ljudi

171
00:10:24,058 --> 00:10:28,105
u isto vreme da bi našao
veoma suptilne paterne.

172
00:10:28,149 --> 00:10:31,369
Uzmite recimo naizgled
nevine digitalne otiske,

173
00:10:31,413 --> 00:10:34,677
kao što je npr. nečija
plejlista na Spotifyju,

174
00:10:34,721 --> 00:10:37,201
ili možda stvari koje
je kupio na Amazonu,

175
00:10:37,245 --> 00:10:40,291
a onda uzmite algoritme
da to prevedu

176
00:10:40,335 --> 00:10:44,513
u jedan veoma detaljan
i vrlo tačan, i intimni profil.

177
00:10:47,603 --> 00:10:50,911
Postoje dosijei svih nas
koji su toliko opsežni

178
00:10:50,954 --> 00:10:52,695
da bi verovatno bilo
tačno reći

179
00:10:52,739 --> 00:10:55,698
da oni znaju više o vama
nego vaša majka.

180
00:11:04,098 --> 00:11:06,883
Glavni razlog
trenutnog razvoja AI-ja

181
00:11:06,927 --> 00:11:08,580
nije to što je neki tamo tip

182
00:11:08,624 --> 00:11:11,583
imao neko briljantno proviđenje,

183
00:11:11,627 --> 00:11:14,325
već mi jednostavno
imamo mnogo više podataka

184
00:11:14,369 --> 00:11:18,242
da na njima treniramo veštačnu inteligenciju
i imamo nesrazmerno bolje kompjutere.

185
00:11:18,286 --> 00:11:19,940
Magija je u podacima.

186
00:11:19,983 --> 00:11:21,463
To je ogromna količina podataka.

187
00:11:21,506 --> 00:11:23,726
Mislim, to su podaci koji 
nikada ranije nisu postojali.

188
00:11:23,770 --> 00:11:26,686
Nikada ranije nismo
imali ovoliko podataka.

189
00:11:26,729 --> 00:11:30,733
Stvorili smo tehnologije
koje nam omogućavaju da prikupimo

190
00:11:30,777 --> 00:11:33,040
ogromne količine informacija.

191
00:11:33,083 --> 00:11:35,738
Kada pomislite na milijarde 
telefona na planeti

192
00:11:35,782 --> 00:11:38,393
sa žiroskopima 
i akcelerometrima

193
00:11:38,436 --> 00:11:39,786
i čitačima otisaka prstiju...

194
00:11:39,829 --> 00:11:42,005
povežite to sa GPS-om
i fotografijama koje ste uslikali

195
00:11:42,049 --> 00:11:43,964
i sa tvitovima koje ste poslali,

196
00:11:44,007 --> 00:11:47,750
svi mi individualno im predajemo 
ogromne količine podataka.

197
00:11:47,794 --> 00:11:50,274
Dok su kola u pokretu, kamere
na njima upijaju informacije

198
00:11:50,318 --> 00:11:52,059
o svetu oko njih.

199
00:11:52,102 --> 00:11:54,844
Sateliti u našoj orbiti
koji su sada veličine tostera.

200
00:11:54,888 --> 00:11:57,629
Infracrveno očitavanje
vegetacije na planeti.

201
00:11:57,673 --> 00:12:01,024
Markeri na površinama okeana
koji šalju info klimatskim simulatorima.

202
00:12:05,072 --> 00:12:08,902
I NSA, i CIA,
dok sakupljaju informacije

203
00:12:08,945 --> 00:12:12,644
o geopolitičkoj situaciji.

204
00:12:12,688 --> 00:12:15,604
Svet u kome živimo danas je 
bukvalno preplavljen podacima.

205
00:12:20,609 --> 00:12:22,480
U 2012. godini,

206
00:12:22,524 --> 00:12:25,875
IBM je procenio
da prosečan čovek

207
00:12:25,919 --> 00:12:31,098
ostavi 500 megabajta
digitalnog otiska svakog dana.

208
00:12:31,141 --> 00:12:34,841
Kada biste želeli da sačuvate
samo količinu podataka za jedan dan

209
00:12:34,884 --> 00:12:36,494
koju stvori čovečanstvo

210
00:12:36,538 --> 00:12:39,062
i da je odštampate
na papir veličine A4,

211
00:12:39,106 --> 00:12:43,806
obostrano, sa veličinom fonta 12,
i kada biste ih složili jedan na drugi,

212
00:12:43,850 --> 00:12:46,113
njegova visina bi bila veća 

213
00:12:46,156 --> 00:12:49,116
Od čeriti razdaljine 
između Zemlje i Sunca.

214
00:12:49,159 --> 00:12:51,292
I tako za svaki dan.

215
00:12:51,335 --> 00:12:53,816
Sami po sebi, podaci
nisu dobri niti zli.

216
00:12:53,860 --> 00:12:55,470
nego zavisi od toga kako se koriste.

217
00:12:55,513 --> 00:12:58,342
Mi se zaista oslanjamo
samo na dobru volju tih ljudi.

218
00:12:58,386 --> 00:13:01,171
i na politike
ovih kompanija.

219
00:13:01,215 --> 00:13:03,870
Ne postoji zakonsko ograničenje
za to kako će oni

220
00:13:03,913 --> 00:13:06,307
da koriste
tu vrstu podataka.

221
00:13:06,350 --> 00:13:09,266
To je, po meni, srž
problema poverenja.

222
00:13:11,007 --> 00:13:13,793
Trenutno je u toku
velika trka da se naprave mašine

223
00:13:13,836 --> 00:13:15,751
koje će biti jednako pametne kao ljudi.

224
00:13:15,795 --> 00:13:18,071
Google -- oni rade na nečemu
što je zapravo reda veličine

225
00:13:18,072 --> 00:13:20,074
Projekta Menhetn
veštačke inteligencije.

226
00:13:20,075 --> 00:13:22,686
Oni imaju najviše novca.
Oni su zaposlili najveće talente.

227
00:13:22,714 --> 00:13:27,067
Oni redom kupuju AI kompanije
i kompanije iz oblasti robotike.

228
00:13:27,110 --> 00:13:29,069
Ljudi i dalje gledaju na
Gogle kao na prosti pretraživač

229
00:13:29,112 --> 00:13:30,722
i na svoj e-mail provajder

230
00:13:30,766 --> 00:13:33,943
isto tako na mnoge druge stvari
koje koristimo svakodnevno,

231
00:13:33,987 --> 00:13:39,383
ali iza tog polja za pretraživanje
stoji 10 miliona servera.

232
00:13:39,427 --> 00:13:43,910
Što čini Google najmoćnijom
računarskom platformom na svetu.

233
00:13:43,953 --> 00:13:47,217
Google trenutno radi
na AI računarskoj platformi

234
00:13:47,261 --> 00:13:50,133
koja će imati
100 milliona servera.

235
00:13:52,179 --> 00:13:53,963
Tako da, kada komunicirate
sa Guglom,

236
00:13:54,007 --> 00:13:56,052
zapravo vidite samo
vrh ledenog brega nečega

237
00:13:56,096 --> 00:13:58,881
što je jedna gigantska zver
u svom nastajanju.

238
00:13:58,925 --> 00:14:00,622
A istina je,
nisam čak ni siguran

239
00:14:00,665 --> 00:14:02,798
da Google zna
u šta se pretvara.

240
00:14:11,546 --> 00:14:15,811
Ako pogledate te algoritme
koji se koriste u Guglu,

241
00:14:15,855 --> 00:14:20,076
to je tehnologija koja je
uglavnom razvijena '80-ih.

242
00:14:20,120 --> 00:14:23,863
Tako da su to modeli koje treniraš
tako što im pokažeš 1, 2,

243
00:14:23,906 --> 00:14:27,344
i 3, a oni nauče ne šta je
1 niti šta je 2 -

244
00:14:27,388 --> 00:14:30,434
Oni nauče koja je razlika
između 1 i 2.

245
00:14:30,478 --> 00:14:32,436
To je samo računanje.

246
00:14:32,480 --> 00:14:35,396
U zadnjoj polovini decenije,
kada smo napravili ovaj nagli razvoj,

247
00:14:35,439 --> 00:14:38,268
svuda se radilo
o prepoznavanju paterna.

248
00:14:38,312 --> 00:14:41,184
Dobri stari AI se sastojao u tome

249
00:14:41,228 --> 00:14:44,057
da naučimo kompjuter

250
00:14:44,100 --> 00:14:46,798
kako da igra igru kao što je šah...

251
00:14:46,842 --> 00:14:49,584
što potiče iz stare paredigme
kada kažeš kompjuteru

252
00:14:49,627 --> 00:14:51,895
tačno šta treba da radi.

253
00:14:54,502 --> 00:14:57,505
Ovo je kviz "Jeopardy!"

254
00:14:59,420 --> 00:15:02,510
"IBM izazov"!

255
00:15:02,553 --> 00:15:05,730
Niko u to vreme nije
nije pretpostavljao da mašina

256
00:15:05,774 --> 00:15:08,298
može imati tu preciznost
i tu pouzdanost

257
00:15:08,342 --> 00:15:11,475
i potrebnu brzinu da igra "Jeopardy!"
dovoljno dobro protiv ljudi.

258
00:15:11,519 --> 00:15:14,609
Igrajmo "Jeopardy!"

259
00:15:18,569 --> 00:15:20,354
Šta je "cipela"?

260
00:15:20,397 --> 00:15:21,877
Pogodili ste.
Sada vi birate.

261
00:15:21,921 --> 00:15:24,836
Lik iz književnosti APB
za $800.

262
00:15:24,880 --> 00:15:28,014
Odgovor -
dnevni bonus.

263
00:15:28,057 --> 00:15:31,539
Watson je svoje znanje stekao
čitajući Wikipediju

264
00:15:31,582 --> 00:15:34,672
i 200 miliona stranica
dokumenata na ljudskim jezicima.

265
00:15:34,716 --> 00:15:36,674
Ne možeš isprogramirati svaku liniju

266
00:15:36,718 --> 00:15:38,502
koda kako funkcioniše svet.

267
00:15:38,546 --> 00:15:40,722
Mašina mora da uči
čitanjem.

268
00:15:40,765 --> 00:15:42,202
Tu dolazimo do Watsona.

269
00:15:42,245 --> 00:15:43,986
"Ko je Bram Stoker?"

270
00:15:44,030 --> 00:15:45,988
A ulog je...?

271
00:15:46,032 --> 00:15:49,165
Zdravo! $17,973.

272
00:15:49,209 --> 00:15:50,993
$41,413.

273
00:15:51,037 --> 00:15:53,343
I zbir za dva dana
sume $77 -

274
00:15:53,387 --> 00:15:56,694
Watson je treniran
na ogromnim količinama teksta,

275
00:15:56,738 --> 00:15:59,628
ali nije da on 
razume šta govori.

276
00:15:59,671 --> 00:16:02,309
On ne zna da stvari postanu mokre
kada dodirnu vodu,

277
00:16:02,352 --> 00:16:04,441
niti vidi kako se 
se stvari ponašaju u svetu

278
00:16:04,485 --> 00:16:06,182
na način na koji vi i ja vidimo.

279
00:16:06,226 --> 00:16:10,143
Većina AI jezka
ne gradi logičke modele

280
00:16:10,186 --> 00:16:11,622
bazirane na tome kako svet funkcioniše.

281
00:16:11,666 --> 00:16:15,365
Već ona vidi
kako se reči pojavljuju

282
00:16:15,409 --> 00:16:18,238
u kontekstu drugih reči.

283
00:16:18,281 --> 00:16:20,196
David Ferrucci
je napravio IBM-ovog Watsona,

284
00:16:20,240 --> 00:16:23,547
i neko ga je pitao,
"Da li Watson misli?"

285
00:16:23,591 --> 00:16:26,660
a on je odgovorio:
"Da li podmornica pliva?"

286
00:16:26,903 --> 00:16:29,331
A ono što je mislio znači,
da kad su pravili podmornice,

287
00:16:29,332 --> 00:16:32,949
pozajmili su bazične principe
plivanja od riba.

288
00:16:33,035 --> 00:16:36,525
Ali podmornica pliva i dalje i brže od 
ribe i može pritom da prenese mnogo više.

289
00:16:36,569 --> 00:16:39,411
Ona ustvari natplivava ribu.

290
00:16:39,955 --> 00:16:43,741
Watsonova pobeda u kvizu "Jeopardy!"
ostaće zapisana u istoriji AI-ja

291
00:16:43,785 --> 00:16:46,370
kao značajna prekratnica.

292
00:16:46,614 --> 00:16:49,269
Često se iznenadimo
kada mašina uradi nešto tako dobro.

293
00:16:49,312 --> 00:16:52,663
Ja sam još više zapanjen kada kompjuter 
pobeđuje čoveka u raznim stvarima

294
00:16:52,707 --> 00:16:55,188
u kojima su ljudi
prirodno jako dobri.

295
00:16:55,231 --> 00:16:58,060
Tako mi pravimo progres.

296
00:16:58,104 --> 00:17:00,671
Ranih dana projekta
Google Brain,

297
00:17:00,715 --> 00:17:02,804
dao sam timu
vrlo prosto uputstvo,

298
00:17:02,847 --> 00:17:05,807
koje je glasilo "Napravite najveću
moguću neuralnu mrežu,

299
00:17:05,850 --> 00:17:08,157
kao što je 1000 kompjutera."

300
00:17:08,201 --> 00:17:12,161
Neuralna mreža je nešto najbliže
simulaciji toga kako radi mozak.

301
00:17:12,205 --> 00:17:16,818
Bazira se na verovatnoći,
ali sa kontekstualnom važnošću.

302
00:17:16,819 --> 00:17:18,456
U vašem mozgu,
imate dugačke neurone

303
00:17:18,457 --> 00:17:20,372
koji su povezani sa hiljadama
drugih neurona,

304
00:17:20,373 --> 00:17:22,592
i imate sve te neke putanje
koje su formirane i učvršćene

305
00:17:22,593 --> 00:17:24,769
i bazirane na tome
šta mozak treba da uradi.

306
00:17:24,782 --> 00:17:28,960
Kada mala beba pokuša nešto
pa uspe, tu je nagrada,

307
00:17:29,004 --> 00:17:32,312
a putanja koja je dovela do 
uspeha je ojačana.

308
00:17:32,355 --> 00:17:34,662
Ako ne uspe u nečemu,
putanja će postati slabija,

309
00:17:34,705 --> 00:17:36,794
i tako, vremenom,
mozak postaje rafiniran

310
00:17:36,838 --> 00:17:40,120
da bude uspešan
u svojoj okolini.

311
00:17:40,363 --> 00:17:43,279
Zaista, radi se samo o tome
da mašine počnu da uče same.

312
00:17:43,323 --> 00:17:45,538
To je "duboko učenje,"
a "duboko učenje"

313
00:17:45,539 --> 00:17:48,834
i "mreže neurona"
su, grubo rečeno, ista stvar.

314
00:17:48,835 --> 00:17:52,391
Duboko učenje
ima potpuno drugačiji pristup

315
00:17:52,419 --> 00:17:55,161
kada kompjuter uči
više kao malo dete,

316
00:17:55,204 --> 00:17:56,466
samo primajući dosta podataka

317
00:17:56,510 --> 00:18:00,340
i na kraju
shvatajući stvari.

318
00:18:00,383 --> 00:18:03,125
Kompjuter tako postaje
samo sve pametniji

319
00:18:03,169 --> 00:18:05,997
kako stiče više iskustva.

320
00:18:06,041 --> 00:18:09,697
Usudite se da zamislite mrežu neurona,
reda veličine od 1000 kompjutera.

321
00:18:09,740 --> 00:18:11,438
Koja se probudi
ne znajući ništa.

322
00:18:11,481 --> 00:18:14,093
A mi joj zadamo da gleda
YouTube nedelju dana.

323
00:18:25,408 --> 00:18:28,194
Čarli!
To je stvarno bolelo!

324
00:18:36,245 --> 00:18:38,508
I tako, nakon gledanja
YouTube-a nedelju dana,

325
00:18:38,552 --> 00:18:39,988
šta će ona naučiti?

326
00:18:40,031 --> 00:18:42,103
Imali smo tu pretpostavku
da će naučiti da uočava

327
00:18:42,146 --> 00:18:44,384
objekte koji se često
pojavljuju u videima.

328
00:18:44,427 --> 00:18:47,517
Dalje, znamo da se ljudsko lice
dosta pojavljuje u videima,

329
00:18:47,561 --> 00:18:49,302
tako da smo to pratili,
i, gle čuda,

330
00:18:49,345 --> 00:18:52,008
tu je bio neuron koji je
naučio da detektuje ljudska lica.

331
00:18:52,052 --> 00:18:55,865
Ostavite Britni na miru!

332
00:18:56,309 --> 00:18:58,354
I, šta se još dosta
pojavljuje u videima?

333
00:19:00,095 --> 00:19:01,792
Proverili smo,
i na naše iznenađenje,

334
00:19:01,836 --> 00:19:05,082
zapravo je je jedan neuron
naučio da uočava mačke.

335
00:19:14,892 --> 00:19:17,068
Još uvek se sećam tog prepoznavanja.

336
00:19:17,112 --> 00:19:20,071
"Wow, to je mačka. Ok, kul.
Super."

337
00:19:23,162 --> 00:19:26,295
Sve se ovo čini vrlo benignim
kada razmišljaš o budućnosti.

338
00:19:26,339 --> 00:19:29,733
Sve deluje 
neškodljivo i dobronamerno.

339
00:19:29,777 --> 00:19:33,520
Ali mi gradimo čitave kognitivne arhtekture
koje će leteti sve brže i brže od nas

340
00:19:33,563 --> 00:19:37,437
i moći će da nose veći teret,
i neće biti slatke i umiljate.

341
00:19:37,480 --> 00:19:41,702
Mislim da ćemo za tri do pet godina,
upoznati kompjuterski sistem

342
00:19:41,745 --> 00:19:45,401
koji će moći da
uči autonomno

343
00:19:45,445 --> 00:19:49,013
kako da razume,
kako da gradi razumevanje,

344
00:19:49,057 --> 00:19:51,364
ne tako različito od načina
na koji ljudski um funkcioniše.

345
00:19:53,931 --> 00:19:56,891
Šta god je taj ručak bio,
vrlo je bilo ukusno.

346
00:19:56,934 --> 00:19:59,807
Prosto, samo neke
Robbyjeve sintetizacije.

347
00:19:59,850 --> 00:20:01,635
On je i tvoj kuvar?

348
00:20:01,678 --> 00:20:04,551
Čak stvara
i sveže namirnice.

349
00:20:04,594 --> 00:20:06,944
Dođi ovamo, Robby.

350
00:20:06,988 --> 00:20:09,773
Pokazaću ti
kako radi.

351
00:20:11,122 --> 00:20:13,342
Stavi se unutra
uzorak ljudske hrane

352
00:20:13,386 --> 00:20:15,344
kroz ovaj otvor.

353
00:20:15,388 --> 00:20:17,738
A unutra je ugrađena
mala hemijska laboratorija,

354
00:20:17,781 --> 00:20:19,218
u kojoj ga on analizira.

355
00:20:19,261 --> 00:20:21,263
Kasnije on može da reprodukuje
identične molekule

356
00:20:21,307 --> 00:20:22,482
u bilo kom obliku i količini.

357
00:20:22,525 --> 00:20:24,614
O, pa to je san svake domaćice.

358
00:20:24,958 --> 00:20:26,834
Upoznajte Baxtera,

359
00:20:26,877 --> 00:20:30,490
novu revolucionarnu vrstu robota,
zdravog razuma.

360
00:20:30,533 --> 00:20:31,839
Baxter...

361
00:20:31,882 --> 00:20:33,449
Baxter je
vrlo dobar primer

362
00:20:33,493 --> 00:20:36,887
kompetitivnosti koja
nas čeka sa mašinama.

363
00:20:36,931 --> 00:20:42,676
Baxter može da uradi skoro sve
što mi možemo sa naše dve ruke.

364
00:20:42,719 --> 00:20:45,722
Baxter nas košta koliko
radinik koji prima minimalac

365
00:20:45,766 --> 00:20:47,507
zaradi za godinu dana.

366
00:20:47,550 --> 00:20:50,318
Ali, Baxter neće zauzeti mesto
radnika sa minimalcem -

367
00:20:50,319 --> 00:20:51,930
On će zauzeti mesto
tri radnika,

368
00:20:51,931 --> 00:20:55,531
zato što se on nikad ne umara,
nikad ne ide na pauzu.

369
00:20:55,558 --> 00:20:57,865
Verovatno je to prvo 
čemu ćemo biti svedoci -

370
00:20:57,908 --> 00:20:59,475
otpuštanja sa poslova.

371
00:20:59,519 --> 00:21:04,088
Mašine će ih raditi
brže, bolje i jeftinije od nas.

372
00:21:04,132 --> 00:21:07,657
Naša moć da budemo u toku
je već veoma ograničena

373
00:21:07,701 --> 00:21:10,138
u poređenju sa
mašinama koje stvaramo.

374
00:21:10,181 --> 00:21:13,446
Na primer, trenutno imamo taj
pokret Ubera i Lyfta

375
00:21:13,489 --> 00:21:16,505
koji se zasniva na pojeftinjenju prevoza
i demokratizaciji transporta,

376
00:21:16,506 --> 00:21:17,768
Što je odlično.

377
00:21:17,769 --> 00:21:21,189
Sledeći korak će se sastojati u tome
da će svi oni biti zamenjeni bespilotnim vozilima

378
00:21:21,192 --> 00:21:25,936
a onda će svi vozači Ubera i Lyfta
morati da nađu nešto novo da rade.

379
00:21:25,980 --> 00:21:29,723
4 miliona je profesionalnih 
vozača u SAD.

380
00:21:29,766 --> 00:21:31,638
Oni će uskoro ostati bez posla.

381
00:21:31,681 --> 00:21:34,075
7 milliona je onih koji 
se bave unosom podataka.

382
00:21:34,118 --> 00:21:37,339
Svi ti ljudi će izgubiti posao.

383
00:21:37,383 --> 00:21:40,342
Posao se ne radi samo radi novca, zar ne?

384
00:21:40,386 --> 00:21:42,605
Na biološkom nivou,
on služi svrsi.

385
00:21:42,649 --> 00:21:45,391
Postaje stvar koja te definiše.

386
00:21:45,434 --> 00:21:48,350
Kada je nestalo poslova
u bilo kojoj datoj civilizaciji,

387
00:21:48,394 --> 00:21:50,987
nije prošlo mnogo dok se to 
nije pretvorilo u masovno nasilje.

388
00:21:59,622 --> 00:22:02,016
Svedoci smo sve dubljeg jaza
između bogatih i siromašnih,

389
00:22:02,059 --> 00:22:05,019
jer to je ono do čega će dovesti automatizacija
i razvoj veštačke inteligencije -

390
00:22:05,062 --> 00:22:08,588
sve veću jaz između
onih koji imaju i onih koji nemaju.

391
00:22:08,631 --> 00:22:10,807
Trenutno, na udrau je 
srednja klasa,

392
00:22:10,851 --> 00:22:12,896
ugroženi su srednjoklasni poslovi.

393
00:22:12,940 --> 00:22:15,334
IBM-ov Watson se bavi
poslovnim analitikama

394
00:22:15,377 --> 00:22:20,600
za koje smo ranije plaćali 
analitičara po $300 na sat.

395
00:22:20,643 --> 00:22:23,037
Danas ti ideš na fakultet
da bi postao doktor,

396
00:22:23,080 --> 00:22:25,082
računovođa, novinar.

397
00:22:25,126 --> 00:22:28,608
Nije sasvim izvesno
da će biti posla za tebe.

398
00:22:28,651 --> 00:22:32,612
Ako neko računa na 
40-ogodišnju karijeru radiologa,

399
00:22:32,655 --> 00:22:34,222
koji samo tumači snimke,

400
00:22:34,265 --> 00:22:37,120
mislim da će to biti izazov
za nove diplomce danas.

401
00:22:58,507 --> 00:23:02,729
Da Vinči robot se trenutno koristi

402
00:23:02,772 --> 00:23:07,516
od strane raznih hirurga
zbog svojih sposobnosti i preciznosti

403
00:23:07,560 --> 00:23:12,303
ako bi se izbegli neizbežni
podrhtaji ljudske ruke.

404
00:23:23,402 --> 00:23:28,494
Svako ko je imao priliku da vidi ovo
osetio je koliko je to fenomenalno.

405
00:23:30,931 --> 00:23:34,674
Pogledaš kroz okular,
i vidiš robotsku kandžu

406
00:23:34,717 --> 00:23:36,893
kako drži jajnik ove žene.

407
00:23:36,937 --> 00:23:42,638
Sudbina čovečanstva je trenutno
u rukama ovog robota.

408
00:23:42,682 --> 00:23:46,947
Kažu da je to budućnost,
Ali ne radi se o budućnosti -

409
00:23:46,990 --> 00:23:50,516
već o sadašnjosti.

410
00:23:50,559 --> 00:23:52,474
Evo na primer imate
hirurškog robota,

411
00:23:52,475 --> 00:23:54,894
obično nema nešto mnogo
inteligencije u ovakvim napravama,

412
00:23:54,895 --> 00:23:58,567
ali vremenom, kako ga budemo hranili
inteligencijom sve više,

413
00:23:58,611 --> 00:24:02,281
taj robot može naučiti
iz svake svoje operacije.

414
00:24:02,284 --> 00:24:04,581
Oni prate kretanja,
i razumevaju

415
00:24:04,582 --> 00:24:06,423
šta je uspelo a šta nije.

416
00:24:06,424 --> 00:24:09,023
I na kraju će takav robot
rutinske operacije

417
00:24:09,024 --> 00:24:12,362
moći potpuno samostalno
da izvede...

418
00:24:12,363 --> 00:24:14,056
ili sa nadgledanjem čoveka.

419
00:24:35,038 --> 00:24:37,214
Izgleda da mi nju
stvaramo i hranimo,

420
00:24:37,258 --> 00:24:42,785
ali na neki način, mi smo
robovi tehnologije,

421
00:24:42,829 --> 00:24:45,701
jer ne možemo da se vratimo na staro.

422
00:24:50,053 --> 00:24:52,882
Mašine uzimaju sve
veće zalogaje

423
00:24:52,926 --> 00:24:57,147
našeg dijapazona veština
brzinom koja se samo povećava.

424
00:24:57,191 --> 00:24:59,236
Tako da i mi moramo biti
sve brži i brži

425
00:24:59,280 --> 00:25:00,890
kako bismo ih prestigli.

426
00:25:02,675 --> 00:25:04,677
Kako izgledam?

427
00:25:04,720 --> 00:25:06,374
Dobro.

428
00:25:10,030 --> 00:25:11,553
Da li sam ti privlačna?

429
00:25:11,597 --> 00:25:14,251
Šta?
- Da li sam ti privlačna?

430
00:25:14,295 --> 00:25:17,777
Daješ mi indikacije da jesam.

431
00:25:17,820 --> 00:25:20,562
Dajem?
- Da.

432
00:25:20,606 --> 00:25:22,608
Ovo je budućnost kojoj hrlimo.

433
00:25:22,651 --> 00:25:26,046
Želimo da kreiramo
svoje saputnike.

434
00:25:26,089 --> 00:25:29,266
Dopada nam se kada vidimo
AI sa ljudskim licem.

435
00:25:29,310 --> 00:25:33,967
i zbog toga će zavarati nas
biti tragićno jednostavno.

436
00:25:34,010 --> 00:25:35,272
Nismo mi toliko komplikovani.

437
00:25:35,316 --> 00:25:38,101
Mi smo jednostavni.
Stimulus-odgovor.

438
00:25:38,145 --> 00:25:42,763
Ja zapravo mogu da te nateram da ti se svidim
samo dosta ti se smešeći.

439
00:25:43,106 --> 00:25:45,974
Veštačka inteligencija će biti odlična
u manipulisanju nama.

440
00:25:54,683 --> 00:25:56,946
Dakle, napravili ste tehnologiju

441
00:25:56,990 --> 00:26:00,036
koja može da oseti
šta tačno ljudi osećaju.

442
00:26:00,080 --> 00:26:03,387
Tako je. Stvorili smo tehnologiju
koja može da pročita vaše facijalne ekspresije

443
00:26:03,431 --> 00:26:06,521
i mapira ih na brojna
emotivna stanja.

444
00:26:06,565 --> 00:26:08,697
Pre 15 godina,
tek što sam završila

445
00:26:08,741 --> 00:26:11,482
osnovne studije
računarskih nauka,

446
00:26:11,526 --> 00:26:15,008
primetila sam da
trošim mnogo vremena

447
00:26:15,051 --> 00:26:17,793
u interakciji sa mojim laptopom
i drugim gedžetima,

448
00:26:17,837 --> 00:26:23,582
a da oni apsolutno nemaju pojma
o tome kako se ja osećam.

449
00:26:23,625 --> 00:26:26,802
I počela sam da razmišljam,
"Šta bi bilo kad bi ova naprava mogla da oseti

450
00:26:26,846 --> 00:26:29,326
da sam ja pod stresom
ili da imam loš dan?

451
00:26:29,370 --> 00:26:31,067
Kakve bi to mogućnosti otvorilo?"

452
00:26:32,721 --> 00:26:34,418
Zdravo, đaci-prvaci!

453
00:26:34,462 --> 00:26:35,855
Kako ste?

454
00:26:35,898 --> 00:26:37,813
Mogu li dobiti zagrljaj?

455
00:26:37,857 --> 00:26:40,773
Naša deca su u 
interakciji sa tehnologijom.

456
00:26:40,816 --> 00:26:44,472
Dosta toga je još uvek u fazi razvoja,
Ali je zapravo neverovatno.

457
00:26:44,515 --> 00:26:46,648
Ko voli robote?
- Ja!

458
00:26:46,692 --> 00:26:48,911
Ko bi voleo da ima robota
u svojoj kući?

459
00:26:48,955 --> 00:26:51,479
Za šta bi ti koristio
robota, Džek?

460
00:26:51,522 --> 00:26:56,353
Koristio bih ga da postavim mami
jako teška pitanja iz matematike.

461
00:26:56,397 --> 00:26:58,181
Ok.
A ti, Teo?

462
00:26:58,225 --> 00:27:02,272
Ja bih ga koristio
za plašenje ljudi.

463
00:27:02,316 --> 00:27:04,666
U redu.
Dakle, počnite sa smejanjem.

464
00:27:04,710 --> 00:27:06,625
Lepo.

465
00:27:06,668 --> 00:27:09,018
Tužne obrve.

466
00:27:09,062 --> 00:27:10,890
Lepo.
Dignite obrve.

467
00:27:10,933 --> 00:27:15,068
Ova generacija je sve vreme
okružena tehnologijom.

468
00:27:15,111 --> 00:27:17,853
Skoro kao da očekuju
da im se pojave roboti u domovima,

469
00:27:17,897 --> 00:27:22,336
a očekuju i da ti roboti
budu socijalno inteligentni.

470
00:27:22,379 --> 00:27:25,252
Šta robota čini pametnim?

471
00:27:25,295 --> 00:27:29,648
Stavite ga recimo na čas 
matematike ili biologije.

472
00:27:29,691 --> 00:27:32,259
Mislim da ćete morati
da ga trenirate.

473
00:27:32,302 --> 00:27:35,218
U redu.
Hajde priđite ovde.

474
00:27:35,262 --> 00:27:37,394
Znači, ako se smejete
ili podignete obrve,

475
00:27:37,438 --> 00:27:39,005
prići će vama.

476
00:27:39,048 --> 00:27:40,833
Prilazi!
On mi prilazi! Gledajte.

477
00:27:43,183 --> 00:27:45,272
Ali ako delujete ljuto,
on će pobeći.

478
00:27:46,534 --> 00:27:48,797
- Neverovatno!
- U, to je bilo super.

479
00:27:48,841 --> 00:27:52,366
Treniramo kompjutere da čitaju
i prepoznaju emocije.

480
00:27:52,409 --> 00:27:53,846
Spremni? Pozor? Sad!

481
00:27:53,889 --> 00:27:57,414
I do sada je fidbek
bio zaista neverovatan.

482
00:27:57,458 --> 00:27:59,590
Ljudi ovo unose
u aplikacije o zdravlju,

483
00:27:59,634 --> 00:28:03,865
o meditaciji, robotima, automobilima.

484
00:28:04,508 --> 00:28:06,728
Videćemo kako će
se to razvijati.

485
00:28:09,470 --> 00:28:11,602
Roboti mogu da sadrže AI,

486
00:28:11,646 --> 00:28:14,388
ali robot je samo
fizička instanca,

487
00:28:14,431 --> 00:28:16,782
a veštačka inteligencija
je mozak.

488
00:28:16,825 --> 00:28:19,872
I ti mozgovi mogu da postoje samo
u sistemima baziranim na softverima.

489
00:28:19,915 --> 00:28:22,483
Oni ne moraju da imaju
fizički oblik.

490
00:28:22,526 --> 00:28:25,094
Roboti mogu da postoje bez bilo kakve
veštačke inteligencije.

491
00:28:25,138 --> 00:28:28,097
Imamo mi i dosta
glupih robota na sve strane.

492
00:28:28,141 --> 00:28:31,753
Ali i glupi robot može
preko noći postati pametan,

493
00:28:31,797 --> 00:28:34,103
ako mu se učita pravi softver,
i pravi senzori.

494
00:28:34,147 --> 00:28:38,629
Ne možemo da odolimo a da ne imputiramo
motive u nežive objekte.

495
00:28:38,673 --> 00:28:41,502
Mi to radimo sa mašinama.
Mi ćemo ih tretirati kao svoju decu.

496
00:28:41,545 --> 00:28:43,330
Tretiraćemo ih kao surogate.

497
00:28:43,373 --> 00:28:45,027
Doviđenja!

498
00:28:45,071 --> 00:28:48,204
I platićemo cenu toga.

499
00:29:08,616 --> 00:29:10,792
Ok, dobrodošli u ATR.

500
00:29:18,000 --> 00:29:20,800
Moj cilj je da imam
više čovekolikog robota

501
00:29:20,801 --> 00:29:24,001
koji ima namere i želje
koje liče na ljudske.

502
00:29:36,000 --> 00:29:38,400
Robot se zove Erica.

503
00:29:39,501 --> 00:29:43,901
I ona je, ja mislim, najnapredniji
humanoidni robot na svetu.

504
00:29:44,202 --> 00:29:47,202
Erika može da te gleda u lice.

505
00:29:51,528 --> 00:29:52,791
Zdravo.

506
00:29:53,592 --> 00:29:57,092
Roboti mogu biti vrlo dobri
kao partneri u razgovoru,

507
00:29:57,093 --> 00:30:00,493
posebno starijima
i mlađoj deci,

508
00:30:00,494 --> 00:30:02,494
i ljudima sa hendikepom.

509
00:30:03,094 --> 00:30:06,294
Dok razgovaramo sa robotom
nismo stegnuti društvenim normama,

510
00:30:06,295 --> 00:30:08,095
socijalnim pritiscima.

511
00:30:08,396 --> 00:30:15,796
I na kraju će svi prihvatiti androida
jednostavno kao svog prijatelja ili kolegu.

512
00:30:15,997 --> 00:30:18,997
Implementirali smo u njih prosta htenja.

513
00:30:18,998 --> 00:30:22,798
Ona je želela da bude prepoznata
i želela je da ode da se odmori.

514
00:30:29,299 --> 00:30:32,299
Ako robot može imati
namere i želje,

515
00:30:32,300 --> 00:30:36,300
taj robot može razumeti namere i
želje ljudi oko njega.

516
00:30:44,300 --> 00:30:47,100
To je u uskoj povezanosti
sa interakcijoma sa ljudima.

517
00:30:47,101 --> 00:30:49,201
a to znači da tu 
postoji međusobno dopadanje.

518
00:30:50,302 --> 00:30:53,302
Što znači, kako bismo rekli, da
oni štite jedni druge.

519
00:30:56,985 --> 00:30:58,682
Mi stvaramo tu veštačku inteligenciju,

520
00:30:58,726 --> 00:31:01,948
i prvo što želimo da postignemo
je da liči na nas.

521
00:31:02,991 --> 00:31:05,341
Mislim da će ključni momenat biti

522
00:31:05,385 --> 00:31:08,858
taj kada osnovna čula
budu replicirana.

523
00:31:09,302 --> 00:31:11,130
Vid...

524
00:31:11,173 --> 00:31:12,871
dodir...

525
00:31:12,914 --> 00:31:14,611
miris.

526
00:31:14,655 --> 00:31:17,919
Kada mu repliciramo naša čula,
da li tada to postaje živo?

527
00:31:27,624 --> 00:31:31,019
Toliko tih naših mašina
je stvoreno kako bi nas razumele.

528
00:31:32,847 --> 00:31:35,005
Ali šta biva kada
antropomorfno stvorenje

529
00:31:35,006 --> 00:31:37,474
otkrije da može da bira
kome daje svoju lojalnost,

530
00:31:37,475 --> 00:31:40,043
kada može da podesi svoju hrabrost,
ili svoju proračunatost,

531
00:31:40,072 --> 00:31:42,291
ili svoju lukavost?

532
00:31:44,859 --> 00:31:48,645
Obični ljudi ne očekuju da će 
ulicama šetati roboti ubice.

533
00:31:48,689 --> 00:31:50,996
Više su u fazonu:
"O čemu vi pričate?"

534
00:31:51,039 --> 00:31:56,245
Čoveče, pa mi želimo da se pobrinemo
da ne bude robota ubica po ulicama.

535
00:31:57,089 --> 00:31:59,439
Jer kada budu po ulicama
već će biti prekasno.

536
00:32:05,053 --> 00:32:08,578
Ono što me trenutno zabrinjava,
i zbog čega ne spavam,

537
00:32:08,622 --> 00:32:11,842
je razvoj
autonomnog oružja.

538
00:32:27,815 --> 00:32:32,733
Dosada su ljudi ispoljavali
nelagodnost zbog razvoja dronova

539
00:32:32,776 --> 00:32:35,127
koji su daljinski
pilotirane letelice.

540
00:32:39,827 --> 00:32:43,309
Ali ako uzmete dronovu kameru
i povežete je sa nekim AI sistemom,

541
00:32:43,352 --> 00:32:47,443
ostaje još samo jedan mali korak
do potpuno autonomnog oružja

542
00:32:47,487 --> 00:32:50,881
koje samo bira svoje mete
i koje samo ispaljuje svoje projektile.

543
00:33:12,729 --> 00:33:15,080
Očekivani životni vek
ljudskog bića

544
00:33:15,123 --> 00:33:19,420
u takvom ratnom okruženju
može biti izmeren u sekundama.

545
00:33:20,563 --> 00:33:23,740
U jednom trenutku, i dronovi
su bili naučna fantastika,

546
00:33:23,784 --> 00:33:28,832
a sada su postali
neizostavni deo ratovanja.

547
00:33:28,876 --> 00:33:33,402
Ima ih 10,000 samo
vojnom inventaru SAD.

548
00:33:33,446 --> 00:33:35,274
ali oni nisu samo
američki fenomen.

549
00:33:35,317 --> 00:33:39,060
Više od 80 zemalja
ih koristi.

550
00:33:39,104 --> 00:33:41,932
Slobodno možemo reći
da ljudi trenutno donose neke od

551
00:33:41,976 --> 00:33:44,587
najvažnijih i najtežih 
odluka na svetu

552
00:33:44,631 --> 00:33:46,328
uz korišćenje
i implementaciju

553
00:33:46,372 --> 00:33:48,591
veštačke inteligencije.

554
00:33:50,767 --> 00:33:53,596
Vazduhoplovstvo je ne tako davno, završilo 
rad na projektu vrednom 400 milijardi dolara

555
00:33:53,640 --> 00:33:55,555
za lansiranje pilota u nebo,

556
00:33:55,598 --> 00:34:01,300
a AI koji je za manje od 1000 dolara,
stvorila nekolicina studenata,

557
00:34:01,343 --> 00:34:03,432
nadjačava i najbolje ljudske pilote

558
00:34:03,476 --> 00:34:05,782
sa svojim relativno
jednostavnim algoritmom.

559
00:34:09,438 --> 00:34:13,399
AI će imati toliko veliki
uticaj na vojsku

560
00:34:13,442 --> 00:34:17,490
koliko je motor sa unutrašnjim sagorevanjem
imao početkom XX veka.

561
00:34:17,533 --> 00:34:21,233
Bukvalno će uticati na sve 
što vojska radi,

562
00:34:21,276 --> 00:34:25,324
od samovozećih konvoja
koji dostavljaju logističke materijale,

563
00:34:25,367 --> 00:34:27,021
do dronova kojima nisu potrebni ljudi

564
00:34:27,065 --> 00:34:30,764
koji dovoze medicinska pomagala,
i to je sračunata propaganda,

565
00:34:30,807 --> 00:34:34,246
kojom će osvajati srca i
umove populacije.

566
00:34:34,289 --> 00:34:38,337
Tako da možemo zaključiti da
ko god bude imao najbolju AI

567
00:34:38,380 --> 00:34:41,688
verovatno će uspeti da 
ostvari dominaciju na planeti.

568
00:34:45,561 --> 00:34:47,650
U jednom trenutku
na samom početku 21. veka,

569
00:34:47,694 --> 00:34:51,219
celo čovečanstvo je 
zajedno slavilo.

570
00:34:51,263 --> 00:34:53,830
Divili smo se 
svojoj sopstvenoj nadmoći

571
00:34:53,874 --> 00:34:56,833
dok smo, u isto vreme, stvarali AI.

572
00:34:56,877 --> 00:34:58,966
AI?

573
00:34:59,009 --> 00:35:00,489
Misliš veštačka inteligencija?

574
00:35:00,533 --> 00:35:01,751
Jedna jedinstvena svest

575
00:35:01,795 --> 00:35:05,886
koja je izrodila
celu jednu populaciju mašina.

576
00:35:05,929 --> 00:35:09,716
Ne znamo ko je zadao prvi 
udarac -- mi ili oni,

577
00:35:09,759 --> 00:35:12,980
ali znamo da smo mi ti
koji su uništili nebo.

578
00:35:14,677 --> 00:35:16,766
Naučna fantastika ima
dugu istoriju,

579
00:35:16,810 --> 00:35:19,987
ne samo u predviđanjem budućnosti,
već i u njenom oblikovanju.

580
00:35:26,863 --> 00:35:30,389
Artur Konan Dojl
je pisao pre Prvog svetskog rata

581
00:35:30,432 --> 00:35:34,393
o opasnosti od korišćenja podmornica

582
00:35:34,436 --> 00:35:38,048
kojima bi se mogle 
sprovoditi pomorske blokade.

583
00:35:38,092 --> 00:35:40,399
U vreme kada je on to pisao,

584
00:35:40,442 --> 00:35:43,402
Kraljevska mornarica je 
ismevala Artura Konana Dojla

585
00:35:43,445 --> 00:35:45,230
zbog ovako besmislene ideje

586
00:35:45,273 --> 00:35:47,623
da podmornice mogu
biti korisne u ratu.

587
00:35:53,455 --> 00:35:55,370
Jedna od tih stvari
o kojima svedoči istorija

588
00:35:55,414 --> 00:35:58,243
je ta da naš stav
prema tehnologiji,

589
00:35:58,286 --> 00:36:01,942
ali tako i etika,
su veoma zavisni od konteksta.

590
00:36:01,985 --> 00:36:03,726
Na primer, podmornica...

591
00:36:03,770 --> 00:36:06,468
zemljama kao što je Velika Britanija
pa čak i SAD

592
00:36:06,512 --> 00:36:09,863
ideja da se koristi podmornica
bila je strašna.

593
00:36:09,906 --> 00:36:13,214
Zapravo, nemačko korišćenje
podmornice da sprovede napade

594
00:36:13,258 --> 00:36:18,480
je bilo povod da se SAD
uključe u Prvi svetski rat.

595
00:36:18,524 --> 00:36:20,613
Ali hajde da pričamo o nekoliko godina kasnije.

596
00:36:20,656 --> 00:36:23,529
Sjedinjene američke države su iznenada

597
00:36:23,572 --> 00:36:28,403
i sa namerom napadnute
od strane Japanskog carstva.

598
00:36:28,447 --> 00:36:32,190
Pet godina nakon Prl Harbora,
izdato je naređenje

599
00:36:32,233 --> 00:36:36,498
da se uđe u podmornički rat
svim silama protiv Japana.

600
00:36:39,936 --> 00:36:43,589
Tako da je Artur Konan Dojl
ipak na kraju bio u pravu.

601
00:36:44,332 --> 00:36:46,856
Ono što se kaže
o naučnoj fantastici --

602
00:36:46,900 --> 00:36:48,336
To je laž koja govori istinu.

603
00:36:48,380 --> 00:36:51,470
Dragi izvršni direktori,
velika mi je čast

604
00:36:51,513 --> 00:36:54,821
da vas uvedem u budućnost
sprovođenja zakona...

605
00:36:54,864 --> 00:36:56,562
ED-209.

606
00:37:03,656 --> 00:37:05,919
Ovo nije samo pitanje
naučne fantastike.

607
00:37:05,962 --> 00:37:09,488
Već se radi o tome šta je sledeće,
i šta se dešava u ovom trenutku.

608
00:37:13,970 --> 00:37:19,324
Zadatag inteligencije je da 
beleži brz rast u ratovanjima.

609
00:37:19,367 --> 00:37:22,152
A svi ulaze u domen 
automatike i bespilotnih mašina.

610
00:37:26,418 --> 00:37:28,898
Ministar odbrane
je vrlo jasan --

611
00:37:28,942 --> 00:37:32,337
Nećemo praviti potpuno
autonomna vozila za napad.

612
00:37:32,380 --> 00:37:34,643
Ali neće se svi
pridržavati

613
00:37:34,687 --> 00:37:36,515
istih vrednosti.

614
00:37:36,558 --> 00:37:40,693
A kada Kina i Rusija počnu
da prave autonomne mašine za rat,

615
00:37:40,736 --> 00:37:45,611
koja mogu da napadaju i ubijaju,
koji je korak na koji ćemo se mi odlučiti?

616
00:37:50,006 --> 00:37:51,617
Ne možete reći,
"Pa, mi ćemo koristiti

617
00:37:51,660 --> 00:37:53,967
autonomna oružja zarad
naše vojnoj dominaciji,

618
00:37:54,010 --> 00:37:56,796
i niko drugi ih
ne može koristiti."

619
00:37:56,839 --> 00:38:00,495
Ako napravite ovo oružje,
ono će biti korišćeno za napad

620
00:38:00,539 --> 00:38:03,324
na ljudsku populaciju
i to u ogromnoj meri.

621
00:38:12,551 --> 00:38:14,596
Autonomna oružja su,
po svojoj prirodi,

622
00:38:14,640 --> 00:38:16,468
oružja masovne destrukcije,

623
00:38:16,511 --> 00:38:19,862
jer njima nije potrebno ljudsko biće
da njima upravlja ili da ih nosi.

624
00:38:19,906 --> 00:38:22,517
Samo vam je jedna osoba 
potrebna da, kako da kažemo,

625
00:38:22,561 --> 00:38:25,781
napiše jedan programčić.

626
00:38:25,825 --> 00:38:30,220
svim ovim obuhvatamo
složenost ovog problema.

627
00:38:30,264 --> 00:38:32,571
Kul je.
Važno je.

628
00:38:32,614 --> 00:38:34,573
Neverovatno je.

629
00:38:34,616 --> 00:38:37,053
Ali je isto tako zastrašujuće.

630
00:38:37,097 --> 00:38:38,968
I sve se bazira na poverenju.

631
00:38:42,102 --> 00:38:44,583
Ovo je otvoreno pismo u vezi sa
veštačkom inteligencijom,

632
00:38:44,626 --> 00:38:47,063
potpisano od strane nekih
od najvećih imena u nauci.

633
00:38:47,107 --> 00:38:48,413
A šta oni žele?

634
00:38:48,456 --> 00:38:50,763
Da se zabrani upotreba
autonomnog oružja.

635
00:38:50,806 --> 00:38:52,373
Autor je izjavio,

636
00:38:52,417 --> 00:38:54,375
"Autonomno oružje
je opisano

637
00:38:54,419 --> 00:38:56,595
kao treća revolucija
u ratovanju."

638
00:38:56,638 --> 00:38:58,853
...hiljade stručnjaka
u oblasti veštačke inteligencije

639
00:38:58,855 --> 00:39:01,875
pozivaju na globalnu 
zabranu robota-ubica.

640
00:39:01,876 --> 00:39:04,357
Ovo otvoreno pismo ustvari poručuje

641
00:39:04,385 --> 00:39:07,954
da treba da redefinišemo ciljeve
u oblasti veštačke inteligencije

642
00:39:07,997 --> 00:39:11,610
umesto razvijanja sirove,
i neusmerene inteligencije,

643
00:39:11,653 --> 00:39:13,655
kako bismo stvorili
dobronamernu inteligenciju.

644
00:39:13,699 --> 00:39:16,092
Ravoj VI-ja
se neće zaustaviti.

645
00:39:16,136 --> 00:39:18,094
Nastaviće se i 
postajaće sve uspešniji.

646
00:39:18,138 --> 00:39:19,835
Ako međunarodna zajednica

647
00:39:19,879 --> 00:39:21,968
ne radi na bilo kakvom
kontrolisanju ovoga,

648
00:39:22,011 --> 00:39:24,666
ljudi će napraviti stvari
koje mogu da urade bilo šta.

649
00:39:24,710 --> 00:39:27,365
Pomenuto pismo kaže da
da smo mi, ne decenijama, već godinama,

650
00:39:27,408 --> 00:39:30,106
udaljeni od upotrebe ovakvog oružja.
Tako da, pre svega...

651
00:39:30,150 --> 00:39:32,413
Imali smo 6,000 potpisnika
tog pisma,

652
00:39:32,457 --> 00:39:35,155
uključujući mnoge od 
važnih figura u ovoj oblasti.

653
00:39:37,026 --> 00:39:39,942
Dosta me posećuju
visokorangirani službenici

654
00:39:39,986 --> 00:39:42,989
koji žele da naglase kako je
vojna dominacija Amerike

655
00:39:43,032 --> 00:39:45,731
veoma bitna,
a autonomno oružje

656
00:39:45,774 --> 00:39:50,083
može biti deo planova
Ministarstva odbrane.

657
00:39:50,126 --> 00:39:52,433
To je jako, jako zastrašujuće,
jer sistem vrednosti

658
00:39:52,477 --> 00:39:54,479
onih koji razvijaju tehnologiju
za vojne potrebe

659
00:39:54,522 --> 00:39:57,307
nije isti kao sistem 
vrednosti ljudske rase.

660
00:40:00,789 --> 00:40:02,922
Bez brige o toj mogućnosti

661
00:40:02,965 --> 00:40:06,665
da ova tehnologija može biti
pretnja ljudskom postojanju,

662
00:40:06,708 --> 00:40:08,144
Grupa naučnika i istraživača

663
00:40:08,188 --> 00:40:09,972
je osnovala
Institut Budućnost života

664
00:40:10,016 --> 00:40:12,192
kako bi se uhvatili u koštac
sa ovim problemima.

665
00:40:13,193 --> 00:40:14,847
Svi ovi momci su tajnoviti,

666
00:40:14,890 --> 00:40:16,805
tako da mi je veoma interesantno
što sam ih opuznao,

667
00:40:16,849 --> 00:40:19,735
tako, na okupu.

668
00:40:20,679 --> 00:40:24,030
Sve što posedujemo je
rezultat naše inteligencije.

669
00:40:24,073 --> 00:40:26,641
Nismo to dobili
uz pomoć velikih strašnih zuba

670
00:40:26,685 --> 00:40:29,470
niti ogromnih kandži
niti velikih mišića.

671
00:40:29,514 --> 00:40:32,473
Već zbog toga što smo
zapravo relativno inteligentni.

672
00:40:32,517 --> 00:40:35,520
A moja generacija,
nama se dešava

673
00:40:35,563 --> 00:40:37,086
ono što se zove 
"wow čoveče",

674
00:40:37,130 --> 00:40:39,045
ili kako se već zovu,
takvi momenti,

675
00:40:39,088 --> 00:40:41,003
jer mi smo svesni
da se tehnologija

676
00:40:41,047 --> 00:40:44,180
ubrzava još brže
nego što smo očekivali.

677
00:40:44,224 --> 00:40:46,705
Sećam se da sam sedeo
za onim tamo stolom

678
00:40:46,748 --> 00:40:50,099
sa nekim od najboljih i 
najvećih umova sveta,

679
00:40:50,143 --> 00:40:52,058
a ono što mi je
najveći utisak ostavilo je,

680
00:40:52,101 --> 00:40:56,149
možda ljudski mozak
ni ne može potpuno da dokuči

681
00:40:56,192 --> 00:40:58,673
kompleksnost sveta
sa kojim smo suočeni.

682
00:40:58,717 --> 00:41:01,415
Kako je do sada pojmljivo,

683
00:41:01,459 --> 00:41:04,766
put AI-ja vodi
direktno ka provaliji,

684
00:41:04,810 --> 00:41:07,595
i mi moramo da promenimo
smer kojim idemo

685
00:41:07,639 --> 00:41:10,729
kako ne bismo ljudsku rasu
odveli pravo u ambis.

686
00:41:13,558 --> 00:41:17,126
Google je kupio DeepMind
pre nekoliko godina.

687
00:41:17,170 --> 00:41:22,088
DeepMind radi kao 
polu-nezavistan Guglova podružnica.

688
00:41:22,131 --> 00:41:24,960
A ono što ga
čini jedinstvenim

689
00:41:25,004 --> 00:41:26,919
je to što je DeepMind
apsolutno fokusiran

690
00:41:26,962 --> 00:41:30,313
na stvaranje digitalne
superinteligencije --

691
00:41:30,357 --> 00:41:34,056
odnosno AI koja je mnogo pametnija
od bilo kog čoveka na Zemlji

692
00:41:34,100 --> 00:41:36,624
i ultimativno pametnija
od svih ljudi na Zemlji zajedno.

693
00:41:36,668 --> 00:41:40,715
Ovo dolazi iz DeepMind-ovog
sistema podržanog učenja.

694
00:41:40,759 --> 00:41:43,544
On se praktično budi
kao novorođena beba

695
00:41:43,588 --> 00:41:46,852
i pokazuje mu se 
neka Atari video igrica

696
00:41:46,895 --> 00:41:50,508
a on onda mora da nauči
da igra tu igricu.

697
00:41:50,551 --> 00:41:55,600
Tada on ne zna ništa o predmetima,
o kretanju, o vremenu.

698
00:41:57,602 --> 00:41:59,604
On samo zna da na ekranu
postoji slika

699
00:41:59,647 --> 00:42:02,563
i da postoji rezultat.

700
00:42:02,607 --> 00:42:06,436
Tako da, kad bi se vaša beba probudila
na dan svog rođenja

701
00:42:06,480 --> 00:42:08,090
i do popodneva,

702
00:42:08,134 --> 00:42:11,093
igrala 40 različitih
Atari igrica

703
00:42:11,137 --> 00:42:15,315
na nivou supermena,
vi biste bili prestravljeni.

704
00:42:15,358 --> 00:42:19,101
I izgovorili biste:
"Moje dete je zaposednuto. Pošaljite ga nazad."

705
00:42:19,145 --> 00:42:23,584
DeepMind sistem
može pobediti u bilo kojoj igri.

706
00:42:23,628 --> 00:42:27,588
Već pobeđuje u svim
originalnim Atari igricama.

707
00:42:27,632 --> 00:42:29,155
On je superioran.

708
00:42:29,198 --> 00:42:31,636
On odigra igricu superbrzinom
za manje od jednog minuta.

709
00:42:37,076 --> 00:42:38,643
DeepMind se okrenuo
jednom drugom izazovu,

710
00:42:38,686 --> 00:42:40,558
a taj izazov je bila
igri Go,

711
00:42:40,601 --> 00:42:42,603
za koju su ljudi 
generalno tvrdili

712
00:42:42,647 --> 00:42:45,084
da je nešto što
kompjuter neće moći da savlada

713
00:42:45,127 --> 00:42:48,304
i DeepMind želi da izazove
najbolje ljudske Go igrače.

714
00:42:48,348 --> 00:42:51,264
Kao prvog, izazvali su
evropskog Go šampiona.

715
00:42:53,222 --> 00:42:55,834
Onda, sledećeg,
korejskog Go šampiona.

716
00:42:55,877 --> 00:42:57,836
Molimo, počnite igru.

717
00:42:57,879 --> 00:42:59,838
I uspeli su da 
pobede oba puta.

718
00:42:59,881 --> 00:43:02,797
i to na vrlo dominantan način.

719
00:43:02,841 --> 00:43:05,017
Pre nekoliko godina mogli ste
da pročitate članke u New York Timesu

720
00:43:05,060 --> 00:43:09,761
koji govore o tome kako će Go
moći da se pobedi tek za 100 godina.

721
00:43:09,804 --> 00:43:13,460
Govorilo se: "Pa, znate,
ali to je još uvek samo tabla.

722
00:43:13,503 --> 00:43:15,027
Poker je umetnost.

723
00:43:15,070 --> 00:43:16,419
U pokeru morate čitati ljude.

724
00:43:16,463 --> 00:43:18,073
Poker sadrži laganje
i blefiranje.

725
00:43:18,117 --> 00:43:19,553
Ne radi se o egzaktnoj stvari.

726
00:43:19,597 --> 00:43:21,381
I to nikad neće moći,
znate već, kompjuter.

727
00:43:21,424 --> 00:43:22,861
"Ne možeš to da uradiš."

728
00:43:22,904 --> 00:43:24,932
Sakupili su najbolje
igrače pokera na svetu,

729
00:43:25,176 --> 00:43:30,520
i kompjuteru je trebalo sedam dana
da počne da uništava ljude u ovoj igri.

730
00:43:30,564 --> 00:43:32,461
Tako da je on najbolji
pokeraš na svetu,

731
00:43:32,462 --> 00:43:35,012
najbolji je igrač Go-a na
svetu, a obrazac koji se ponavlja

732
00:43:35,013 --> 00:43:37,454
je taj da će AI-ju
možda trebati malo vremena

733
00:43:37,484 --> 00:43:40,443
kako bi obavio svoje pipke
oko nove veštine,

734
00:43:40,487 --> 00:43:44,883
ali kada to uradi i kada
skapira, nezaustavljiv je.

735
00:43:52,020 --> 00:43:55,110
DeepMind AI ima
admin pristup

736
00:43:55,154 --> 00:43:57,156
Googleovim serverima

737
00:43:57,199 --> 00:44:00,768
kako bi optimizovao korišćenje
centara baza podataka.

738
00:44:00,812 --> 00:44:04,816
Međutim, to bi moglo biti
nešto kao nenamerni Trojanski konj.

739
00:44:04,859 --> 00:44:07,253
DeepMind mora imati potpunu 
kontrolu centrala podataka,

740
00:44:07,296 --> 00:44:08,950
tako da, uz vrlo mali
apdejt softvera,

741
00:44:08,994 --> 00:44:10,691
ta AI bi mogla da
preuzme potpunu kontrolu

742
00:44:10,735 --> 00:44:12,214
na delim Google-ovim sistemom,

743
00:44:12,258 --> 00:44:13,607
Što znači da ima
neograničene mogućnosti.

744
00:44:13,651 --> 00:44:16,131
Može da vidi sve vaše podatke.
Može da uradi bilo šta.

745
00:44:20,135 --> 00:44:23,051
Ubrzano se krećemo ka eri
digitalne superinteligencije

746
00:44:23,095 --> 00:44:24,313
koja daleko premašuje ljudsku.

747
00:44:24,357 --> 00:44:26,402
Mislim da je vrlo očigledno.

748
00:44:26,446 --> 00:44:29,710
Problem je taj što mi nećemo
tek tako doći do granica ljudske inteligencije

749
00:44:29,754 --> 00:44:33,105
i reći:
"To je to, ne idemo dalje."

750
00:44:33,148 --> 00:44:35,015
Naravno da ćemo ići iznad
ljudskog nivoa inteligencije

751
00:44:35,016 --> 00:44:39,459
sve do onoga što se zove "superintelligencija",
a to bi bilo šta god je pametnije od nas.

752
00:44:39,502 --> 00:44:42,810
VI na nivou superčoveka,
ako u tome uspemo, biće to

753
00:44:42,854 --> 00:44:46,553
daleko najmoćniji
pronalazak ikada

754
00:44:46,596 --> 00:44:50,296
a ujedno i poslednji
pronalazak koji ćemo morati na napravimo.

755
00:44:50,339 --> 00:44:53,168
I ako stvorimo AI
koja je pametnija od nas,

756
00:44:53,212 --> 00:44:54,735
moramo biti otvoreni
za tu mogućnost

757
00:44:54,779 --> 00:44:57,520
da ćemo mi zapravo
izgubiti kontrolu nad njom.

758
00:45:00,785 --> 00:45:02,612
Hajde da kažemo
da joj damo neki cilj,

759
00:45:02,656 --> 00:45:04,745
kao što je lečenje raka,
a onda otkrijemo

760
00:45:04,789 --> 00:45:06,965
da način na koji ona
bira da dođe do tog cilja

761
00:45:07,008 --> 00:45:08,444
je zapravo u konfliktu

762
00:45:08,488 --> 00:45:11,705
sa mnogim drugim stvarima
do kojih nam je stalo.

763
00:45:12,448 --> 00:45:16,496
AI ne mora da bude zla
da bi uništila čovečanstvo.

764
00:45:16,539 --> 00:45:20,674
Ako AI ima neki cilj, a čovečanstvo
je slučajno na putu do njega,

765
00:45:20,718 --> 00:45:22,894
Uništiće ga rutinski,

766
00:45:22,937 --> 00:45:25,113
Bez razmišljanja o tome.
Nema ljutnje.

767
00:45:25,157 --> 00:45:27,072
To je isto kao
npr, kad gradimo put,

768
00:45:27,115 --> 00:45:29,770
i desi se da se na tom
putu nađe mravinjak...

769
00:45:29,814 --> 00:45:31,467
Ne mrzimo mi mrave...

770
00:45:31,511 --> 00:45:33,165
Mi samo gradimo put.

771
00:45:33,208 --> 00:45:34,857
Tako da, doviđenja mravinjaku.

772
00:45:37,996 --> 00:45:40,172
Vrlo je privlačno
ne razmišljati o ovim problemima,

773
00:45:40,215 --> 00:45:42,783
'jer je to nešto što, kao,
može da se desi

774
00:45:42,827 --> 00:45:47,396
za nekoliko decenija ili 100 godina,
pa zašto bismo brinuli?

775
00:45:47,440 --> 00:45:50,704
Ali ako se osvrnete na
11 septembar 1933,

776
00:45:50,748 --> 00:45:54,795
Ernest Rutherford, koji je bio
najpoznatiji nuklearni fizičar u svoje vreme,

777
00:45:54,839 --> 00:45:58,668
rekao je da je mogućnost
izvlačenja korisne količine energije

778
00:45:58,712 --> 00:46:00,801
iz transmutacije atoma,
kako je on to nazvao,

779
00:46:00,845 --> 00:46:03,151
zapravo, alhemija.

780
00:46:03,195 --> 00:46:06,502
Sledećeg jutra, Leo Szilard,
koji je bio mnogo mlađi fizičar,

781
00:46:06,546 --> 00:46:09,984
pročitao je ovo, i to ga je jako
iznerviralo, pa je shvatio

782
00:46:10,028 --> 00:46:11,943
kako da napravi
nuklearnu lančanu reakciju

783
00:46:11,986 --> 00:46:13,379
samo par meseci kasnije.

784
00:46:20,603 --> 00:46:23,693
Potrošili smo više
od 2 milijarde

785
00:46:23,737 --> 00:46:27,523
na najveće naučno
kockanje u istoriji.

786
00:46:27,567 --> 00:46:30,222
Tako da kada neko kaže,
"O, to je sve u tako

787
00:46:30,265 --> 00:46:32,528
dalekoj budućnosti, ne moramo
da brinemo o tome",

788
00:46:32,572 --> 00:46:36,271
možda će biti potrebno tek dva-tri
naučna proboja tog reda veličine

789
00:46:36,315 --> 00:46:40,275
da nas dovedu odavde
do superinteligentnih mašina.

790
00:46:40,319 --> 00:46:42,974
Ako će biti potrebno
20 godina da se shvati

791
00:46:43,017 --> 00:46:45,237
kako da AI ostane dobronamerna,

792
00:46:45,280 --> 00:46:48,849
onda treba da počnemo danas,
sa time, a ne u poslednjem trenutku

793
00:46:48,893 --> 00:46:51,460
kada neke batice
koji piju Red Bull

794
00:46:51,504 --> 00:46:53,832
odluče da stisnu prekidač
i testiraju to stvorenje.

795
00:46:56,814 --> 00:46:58,859
Imamo pet godina.

796
00:46:58,903 --> 00:47:03,764
Mislim da će se do digitalne superinteligencije 
doći za vreme mog života.

797
00:47:03,908 --> 00:47:05,735
100%.

798
00:47:05,779 --> 00:47:07,215
A kad se to dogodi,

799
00:47:07,259 --> 00:47:09,696
ona ćebiti okružena
gomilom ljudi

800
00:47:09,739 --> 00:47:13,091
koji će biti samo
fanatici za tehnologiju.

801
00:47:13,134 --> 00:47:15,571
Oni žele da vide da ona uspe,
i njima ne pada na pamet

802
00:47:15,615 --> 00:47:16,964
da bi mogla da se otrgne kontroli.

803
00:47:25,494 --> 00:47:28,584
O, Bože, ja toliko
verujem svom kompjuteru.

804
00:47:28,628 --> 00:47:30,195
To je tako dobro pitanje.

805
00:47:30,238 --> 00:47:31,457
Ja ne verujem svome kompjuteru.

806
00:47:31,500 --> 00:47:32,937
Ako je ostao uključen,
isključim ga.

807
00:47:32,980 --> 00:47:34,242
Ali, kao, čak i kad je isključen,

808
00:47:34,286 --> 00:47:35,896
Ja i dalje mislim da je uključen.
Razumeš me?

809
00:47:35,897 --> 00:47:37,694
Kao, ne možeš zaista da veru--
kao, evo web kamere...

810
00:47:37,695 --> 00:47:39,625
Ne znaš da li će je možda, 
kao, neko upaliti...

811
00:47:39,639 --> 00:47:41,249
Kao, ne znaš.

812
00:47:41,293 --> 00:47:42,903
Ja ne verujem svome kompjuteru.

813
00:47:42,947 --> 00:47:46,907
Kao, u mom telefonu,
svaki put me pita

814
00:47:46,951 --> 00:47:49,475
"Možemo li poslati
informaciju o vama Apple-u?"

815
00:47:49,518 --> 00:47:50,998
svaki put, ja...

816
00:47:51,042 --> 00:47:53,087
Tako da, ne verujem svom telefonu.

817
00:47:53,131 --> 00:47:56,743
Ok. Delom, da, verujem mu,

818
00:47:56,786 --> 00:48:00,660
pošto bi bilo jako teško
izgurati dan

819
00:48:00,703 --> 00:48:04,011
u svetu koji se toliko bazira
na kompjuterskim postavkama.

820
00:48:10,975 --> 00:48:13,368
Poverenje je jedno tako ljudsko iskustvo.

821
00:48:21,289 --> 00:48:25,119
Dolaze mi pacijenti
sa aneurizmom mozga.

822
00:48:30,037 --> 00:48:31,691
Oni žele da me pogledaju u oči
i da saznaju od mene

823
00:48:31,734 --> 00:48:34,955
da mogu meni kao osobi
poveriti svoje živote.

824
00:48:34,999 --> 00:48:39,129
Nisam nešto preterano
zabrinuta.

825
00:48:39,138 --> 00:48:40,204
Dobro.

826
00:48:40,206 --> 00:48:42,920
Delom jer imam 
poverenja u Vas.

827
00:48:50,753 --> 00:48:57,151
Proceduru koju izvodimo danas,
pre 20 godina nije bilo moguće sprovesti.

828
00:48:57,195 --> 00:49:00,328
Jednostavno nismo imali
odgovarajući materijal i tehnologiju.

829
00:49:22,698 --> 00:49:26,485
Tako da, traka
je tek nešto malo unutra.

830
00:49:26,528 --> 00:49:29,923
Samo jedno pero
je drži unutra.

831
00:49:29,967 --> 00:49:32,012
Situacija je napeta.

832
00:49:36,190 --> 00:49:40,673
Trenutno smo u čistilištu,
intelektualnom, humanističkom čistilištu,

833
00:49:40,716 --> 00:49:43,632
a AI bi možda i znala
kakvu odluku treba doneti.

834
00:49:50,639 --> 00:49:52,554
Smestili smo namotaj u aneurizmu.

835
00:49:52,598 --> 00:49:54,556
Ali nije legla unutra
baš najbolje

836
00:49:54,600 --> 00:49:56,428
što bi mi dozvolilo da ga ostavim tamo,

837
00:49:56,471 --> 00:50:01,041
tako da imamo 20% rizika
od veoma lošeg ishoda,

838
00:50:01,085 --> 00:50:04,436
pa sam izabrao
da je jednostavno vratim.

839
00:50:04,479 --> 00:50:05,959
Najviše zbog mojeg
ljudskog odnosa sa njom

840
00:50:06,003 --> 00:50:08,222
ai znajući za poteškoće
njenog dolaska

841
00:50:08,266 --> 00:50:11,051
a dok sam radio zahvat,
razmatrao sam opcije,

842
00:50:11,095 --> 00:50:14,272
kada treba uzeti u obzir
samo najbezbedniju opciju

843
00:50:14,315 --> 00:50:16,361
kako bi dostigli uspeh.

844
00:50:16,404 --> 00:50:19,755
Međutim, morao sam jedno 10 
minuta da provedem u agoniji.

845
00:50:19,799 --> 00:50:21,757
A kompjuter ne oseća ništa.

846
00:50:21,801 --> 00:50:24,760
On samo uradi
što bi trebalo da uradi,

847
00:50:24,804 --> 00:50:26,284
sve bolje i bolje.

848
00:50:30,331 --> 00:50:32,551
U ovom slučaju, voleo bih da sam AI.

849
00:50:35,945 --> 00:50:38,861
Ali, da li AI može saosećati?

850
00:50:43,083 --> 00:50:47,827
Mislim, svi pitaju isto 
to pitanje o AI-ju.

851
00:50:47,870 --> 00:50:51,961
Mi smo jedino
otelotvorenje ljudskog roda,

852
00:50:52,005 --> 00:50:55,269
i nije nam lako
da prihvatimo da mašina

853
00:50:55,313 --> 00:50:58,794
Može imati saosećanja
i ljubavi na isti način.

854
00:51:05,149 --> 00:51:07,281
Deo mene ne veruje u magiju,

855
00:51:07,325 --> 00:51:09,805
ali dao mene veruje
da postoji nešto

856
00:51:09,849 --> 00:51:11,546
izvan proste logike,

857
00:51:11,590 --> 00:51:15,637
da postoji nešto jedno
u našem zajedničkom poreklu,

858
00:51:15,681 --> 00:51:19,738
zajednička biologija,
zajednička istorija.

859
00:51:20,381 --> 00:51:23,210
Neka veza postoji
nedokučiva mašinama.

860
00:51:30,348 --> 00:51:32,567
I tako imate i 
i drugu stranu toga,

861
00:51:32,611 --> 00:51:37,137
a to je da li kompjuter ima saznanje o svojoj svesti, ili da li može biti svestan i da li ga briga?

862
00:51:37,181 --> 00:51:40,009
Da li treba da ima svest?

863
00:51:40,053 --> 00:51:42,011
Da li mu je potrebno da bude svestan?

864
00:51:52,892 --> 00:51:56,417
Mislim da robot nikad
ne može imati svest.

865
00:51:56,461 --> 00:51:58,376
Sem ako nije programiran na taj način.

866
00:51:58,419 --> 00:52:00,639
Svestan? Ne.

867
00:52:00,682 --> 00:52:03,163
Ne.

868
00:52:03,207 --> 00:52:06,035
Mislim, robot može biti
programiran da bude svestan.

869
00:52:06,079 --> 00:52:09,648
Kako su inače programirani
da rade sve ostalo?

870
00:52:09,691 --> 00:52:12,390
To je još jedan veliki deo
veštačke inteligencije,

871
00:52:12,433 --> 00:52:15,741
da ih učine svesnima
i omoguće im da osećaju.

872
00:52:22,443 --> 00:52:27,709
2005 smo započeli pokušaje na
pravljenju mašina sa samosvešću.

873
00:52:33,106 --> 00:52:37,284
Za početak, ovaj robot,
nije imao svest o sebi.

874
00:52:37,328 --> 00:52:40,244
Sve što je znao bilo je to
da treba da se kreće.

875
00:52:44,117 --> 00:52:45,597
Kroz pokušaje i pogreške,

876
00:52:45,640 --> 00:52:49,731
shvatio je kako da hoda
koristeći imaginaciju,

877
00:52:49,775 --> 00:52:54,040
a onda je otišao.

878
00:52:54,083 --> 00:52:56,390
A onda smo mi uradili
nešto jako okrutno.

879
00:52:56,434 --> 00:52:58,653
Odstranili smo mu nogu
i gledali šta će da se desi.

880
00:53:03,049 --> 00:53:07,749
U početku mu nije bilo sasvim
jasno šta se desilo.

881
00:53:07,793 --> 00:53:13,233
Ali tokom dana
počeo je da hramlje.

882
00:53:13,277 --> 00:53:16,845
A onda smo, pre godinu dana
trenirali jedan AI sistem

883
00:53:16,889 --> 00:53:20,240
za demonstraciju uživo.

884
00:53:20,284 --> 00:53:24,113
Želeli smo da pokažemo
kako sve ove predmete ispred kamere

885
00:53:24,157 --> 00:53:27,334
AI može da prepozna.

886
00:53:27,378 --> 00:53:29,031
I tako, pripremali
smo taj demo,

887
00:53:29,075 --> 00:53:31,251
a na ekranu sa strane
mogli smo

888
00:53:31,295 --> 00:53:36,778
da vidimo na šta reaguju
određeni neuroni.

889
00:53:36,822 --> 00:53:41,087
I odjednom smo primetili da jedan
od neurona prati lica.

890
00:53:41,130 --> 00:53:45,483
Pratio je naša lica
dok smo se mi kretali okolo.

891
00:53:45,526 --> 00:53:48,616
E sad, jezivo je bilo to
što nikad nismo trenirali

892
00:53:48,660 --> 00:53:52,490
taj sistem da prepoznaje ljudska lica,

893
00:53:52,533 --> 00:53:55,710
a on je eto, nekako
naučio to da radi.

894
00:53:57,973 --> 00:53:59,784
Iako su ovi roboti
vrlo jednostavni,

895
00:53:59,785 --> 00:54:02,658
Možemo videti da se još
nešto tu dešava.

896
00:54:02,659 --> 00:54:05,867
I nije u pitanju samo programiranje.

897
00:54:05,894 --> 00:54:08,462
Tako da, ovo je samo početak.

898
00:54:10,377 --> 00:54:14,294
Često mislim o toj
plaži u Kitty Hawk-u.

899
00:54:14,338 --> 00:54:18,255
Let 1903 izveden od strane 
Orvilla and Wilbura Wrighta.

900
00:54:21,214 --> 00:54:24,289
a letelica napravljena od platna,
i tu je nešto gvožđa i drveta,

901
00:54:24,291 --> 00:54:26,928
i počinje da uzleće za,
koliko, minute i 20 sekundi,

902
00:54:26,929 --> 00:54:31,006
tog vetrovitog dana,
pre nego što je opet dodirnula zemlju.

903
00:54:33,270 --> 00:54:37,143
A  desilo se da posle samo
65 leta ili tu negde, 

904
00:54:37,186 --> 00:54:43,149
nakon tog momenta imate avion,
747 koji uzleće sa aerodroma JFK...

905
00:54:50,099 --> 00:54:52,184
...u kome je najveća briga
nekoga u avionu to

906
00:54:52,185 --> 00:54:55,380
da li njegov
dijetalni obrok bez soli

907
00:54:55,381 --> 00:54:56,917
stiže njemu ili ne.

908
00:54:56,945 --> 00:55:01,385
Imamo čitavu infrastrukturu, sa sve
turističkim agencijama i kontrolnim tornjem,

909
00:55:01,428 --> 00:55:03,778
i sve je to opušteno,
sve je to deo stvarnosti.

910
00:55:07,086 --> 00:55:09,523
A sada, koliko god da smo
napredovali sa mašinama

911
00:55:09,567 --> 00:55:12,134
koje mogu da misle i reše problem,
mi smo zapravo na stupnju Kitty Hawk-a.

912
00:55:12,178 --> 00:55:13,745
U vazduhu smo.

913
00:55:13,788 --> 00:55:17,052
Uzletele su naše
letelice sa jedrima.

914
00:55:20,926 --> 00:55:23,885
Ali šta će se desiti
za 65 leta?

915
00:55:23,929 --> 00:55:27,889
Imaćemo mašine koje će biti
izvan kontrole ljudi.

916
00:55:27,933 --> 00:55:30,457
Treba li to da nas brine?

917
00:55:32,633 --> 00:55:34,853
Ne znam baš da li će nam to pomoći.

918
00:55:40,337 --> 00:55:44,036
Niko danas ne može ni da 
pretpostavi šta za jednog robota

919
00:55:44,079 --> 00:55:46,430
znači da ima svest.

920
00:55:46,473 --> 00:55:48,649
Takva stvar ne postoji.

921
00:55:48,693 --> 00:55:50,172
Mnogo je pametnih ljudi,

922
00:55:50,216 --> 00:55:53,088
i ja ih zaista
dosta poštujem,

923
00:55:53,132 --> 00:55:57,528
ali istina je da su
mašine po svojoj prirodi psihopate.

924
00:55:57,571 --> 00:55:59,225
Strah je vraćen na tržište.

925
00:55:59,268 --> 00:56:01,706
Palo je na 800,
skoro 1,000, u sekundi.

926
00:56:01,749 --> 00:56:03,360
Mislim,
to je klasična kapitulacija.

927
00:56:03,403 --> 00:56:07,146
Ima ljudi koji misle da je za to
krivac nečiji debeli prst.

928
00:56:07,189 --> 00:56:09,583
Uzmite za primer Flash Crash iz 2010.

929
00:56:09,627 --> 00:56:13,413
Za samo jedan minut,
trilion dolara vrednosti

930
00:56:13,457 --> 00:56:15,415
izgubljeno je na berzi.

931
00:56:15,459 --> 00:56:18,984
The Dow je izgubio vrednost od
1,000 poena u roku od pola sata.

932
00:56:19,027 --> 00:56:22,553
Šta je pošlo naopako?

933
00:56:22,596 --> 00:56:26,644
Sve do tog trenutka,
više od 60% sve trgovine

934
00:56:26,687 --> 00:56:29,124
koja se odigravala
na berzi

935
00:56:29,168 --> 00:56:32,693
bilo je zapravo
započeto od strane kompjutera.

936
00:56:32,737 --> 00:56:35,783
Kako je opadala vrednost, desilo se panično
prodavanje i najednom se zaustavilo na centu.

937
00:56:35,827 --> 00:56:37,611
Sve se ovo dešava u 
realnom vremenu, narode.

938
00:56:37,612 --> 00:56:39,883
Ukratko vam pričam kako
se dogodio Flash Crash

939
00:56:39,884 --> 00:56:42,513
a to je da su algortmi
odgovorili na algoritme,

940
00:56:42,514 --> 00:56:45,430
i oni su se tako iznova i 
iznova sračunavali

941
00:56:45,431 --> 00:56:47,041
sve za nekoliko minuta.

942
00:56:47,055 --> 00:56:50,972
U jednom trenutku, celo
tržište je palo, kao u ambis.

943
00:56:51,016 --> 00:56:54,323
Ne postoji ni regulator
koji može toliko brzo da se prilagodi

944
00:56:54,367 --> 00:56:57,979
kako bi predupredio potencijalne
katastrofalne posledice

945
00:56:58,023 --> 00:57:01,243
koje bi AI napravio
u našem finansijskom ustrojstvu.

946
00:57:01,287 --> 00:57:03,898
Ono je toliko
podložno manipulaciji.

947
00:57:03,942 --> 00:57:05,639
Hajde da pričamo o
brzini kojom

948
00:57:05,683 --> 00:57:08,076
ovo tržište degradira.

949
00:57:08,120 --> 00:57:11,602
To je vrsta VI mahnitosti
koja ljude plaši.

950
00:57:11,645 --> 00:57:13,560
Kada joj date cilj,

951
00:57:13,604 --> 00:57:17,225
ona će ga neumorno juriti.

952
00:57:17,869 --> 00:57:20,393
Koliko imamo takvih
kompjuterskih programa?

953
00:57:20,437 --> 00:57:22,683
Niko to ne zna.

954
00:57:23,527 --> 00:57:27,444
Jedan od fascinantnih aspekata
o AI-ju generalno

955
00:57:27,487 --> 00:57:31,970
je taj da više niko zaista
ne razume kako ona funkcioniše.

956
00:57:32,013 --> 00:57:36,975
Čak ni ljudi koji su stvorili AI
ne razumeju je potpuno.

957
00:57:37,018 --> 00:57:41,675
Jer ona sadrži milione elemenata,
pa postaje potpuno nemoguće

958
00:57:41,719 --> 00:57:45,113
za ljudsko biće
da shvati šta se tu dešava.

959
00:57:52,556 --> 00:57:56,037
Microsoft je dao postavku
ove veštačke inteligencije

960
00:57:56,081 --> 00:57:59,127
koja se zove Tay na Twitteru,
koji je čet bot.

961
00:58:00,912 --> 00:58:02,696
Počeli su ujutru,

962
00:58:02,740 --> 00:58:06,526
a Tay je počinjao da tvituje
i da uči kroz stvari

963
00:58:06,570 --> 00:58:10,835
koje su mu poslali
drugi ljudi sa Twittera.

964
00:58:10,878 --> 00:58:13,272
Ali, pšto su ga neki ljudi,
kao npr. trolovi, napadali,

965
00:58:13,315 --> 00:58:18,582
za 24 časa, Microsoftov bot
postao je jedna užasna osoba.

966
00:58:18,625 --> 00:58:21,367
Morali su bukvalno
da ga povuku sa neta

967
00:58:21,410 --> 00:58:24,718
jer se pretvorio u čudovište.

968
00:58:24,762 --> 00:58:30,550
Mizantropa, rasistu, užasnu osobu
koju nikad ne biste poželeli da upoznate.

969
00:58:30,594 --> 00:58:32,857
I niko ovo nije predvideo.

970
00:58:35,337 --> 00:58:38,602
Cela ideja VA-ja je ta 
da joj mi ne govorimo kako

971
00:58:38,645 --> 00:58:42,780
tačno da postigne cilj.

972
00:58:42,823 --> 00:58:46,435
AI se razvija sama od sebe.

973
00:58:46,479 --> 00:58:48,829
Mi se plašimo
superinteligentne AI,

974
00:58:48,873 --> 00:58:52,790
kao nekog šahovskog majstora
koji će nas preveslati,

975
00:58:52,833 --> 00:58:55,923
ali AI zapravo neće morati
da bude toliko pametna

976
00:58:55,967 --> 00:59:00,145
kako bi imala masovno nepogodne
efekte na ljudsku civilizaciju.

977
00:59:00,188 --> 00:59:01,886
Tokom prošlog veka smo videli

978
00:59:01,929 --> 00:59:05,150
da nije nužno potrebno da ste genije
kako bi uništili istorijske tekovine

979
00:59:05,193 --> 00:59:06,804
u određenom smeru,

980
00:59:06,847 --> 00:59:09,589
niti će biti potreban AI genije
da uradi istu stvar.

981
00:59:09,633 --> 00:59:13,158
Tokom predizborne kampanje, lažne vesti
su generisale više fidbeka

982
00:59:13,201 --> 00:59:17,075
na Facebook-u od najvažnijih pravih vesti.

983
00:59:17,118 --> 00:59:21,079
Facebook je zaista jedan
najočigledniji prećutni primer.

984
00:59:21,122 --> 00:59:23,777
AI obavlja Facebookov dotok vesti--

985
00:59:23,821 --> 00:59:28,347
Zadatak te AI
je da natera korisnike da reaguju,

986
00:59:28,390 --> 00:59:29,827
ali niko zaista ne razume

987
00:59:29,870 --> 00:59:34,832
na koji način AI
dolazi do ovog cilja.

988
00:59:34,875 --> 00:59:38,792
Facebook diže jedan elegantni
zid od ogledala oko nas.

989
00:59:38,836 --> 00:59:41,665
Ogledalo koje možeš da pitaš,
"Ko je najlepši na svetu?"

990
00:59:41,708 --> 00:59:45,016
i ono će odgovoriti: "Ti, ti,"
svaki put

991
00:59:45,059 --> 00:59:48,193
i tako će polako počinjati
da menja naš osećaj za realnost,

992
00:59:48,236 --> 00:59:53,502
a onda iskriviti i osećaj za politiku,
istoriju, globalne događaje,

993
00:59:53,546 --> 00:59:57,028
dok će utvrđivanje toga
šta je stvarno a šta ne,

994
00:59:57,071 --> 00:59:58,943
biti skroz nemoguće.

995
01:00:01,032 --> 01:00:03,861
Problem je što AI
to ne razume.

996
01:00:03,904 --> 01:00:08,039
AI je samo imala sledeću misiju --
maksimizuj angažovanje korisnika,

997
01:00:08,082 --> 01:00:10,041
i ona je u tome uspela.

998
01:00:10,084 --> 01:00:13,653
Skoro 2 milijarde ljudi
potroši skoro sat vremena

999
01:00:13,697 --> 01:00:17,831
u proseku dnevno
bukvalno u interakciji sa AI-jem

1000
01:00:17,875 --> 01:00:21,530
koji oblikuje
njihova iskustva.

1001
01:00:21,574 --> 01:00:24,664
Čak ni Facebookovi inžinjeri
ne vole lažne vesti.

1002
01:00:24,708 --> 01:00:28,015
To je loš biznis.
Oni hoće da se otarase tih lažnih vesti.

1003
01:00:28,059 --> 01:00:32,324
Ali to je veoma teško sprovesti,
jer kako da prepoznaš vestkao lažnu

1004
01:00:32,367 --> 01:00:34,456
ako ne možeš ti lično
da čitaš sve te vesti?

1005
01:00:34,500 --> 01:00:39,418
Toliko je aktivnih
dezinformavija

1006
01:00:39,461 --> 01:00:41,115
koje su vrlo dobro upakovane,

1007
01:00:41,159 --> 01:00:44,553
a izgledaju isto
kao što ih vidite na Facebook stranici

1008
01:00:44,597 --> 01:00:47,426
ili kako ih vidite na televiziji.

1009
01:00:47,469 --> 01:00:51,691
Nije bog zna kako sofisticirano,
ali je neverovatno moćno.

1010
01:00:51,735 --> 01:00:54,346
A to znači da je
vaš pogled na svet,

1011
01:00:54,389 --> 01:00:56,435
koji je, pre 20 godina
bio određen,

1012
01:00:56,478 --> 01:01:00,004
ako ste pratili večernje vesti,
tri različite mreže,

1013
01:01:00,047 --> 01:01:02,528
a to su bila trojica izveštača
koji su se trudili da objektivno izveštavaju.

1014
01:01:02,529 --> 01:01:04,583
Možda su bili malo pristrasni
na ovaj ili onaj način,

1015
01:01:04,584 --> 01:01:08,273
ali, uglavnom se možemo složiti
oko objektivne realnosti.

1016
01:01:08,316 --> 01:01:10,754
E pa, ta objektivnost je nestala,

1017
01:01:10,797 --> 01:01:13,757
a Facebook ju je
potpuno uništio.

1018
01:01:17,108 --> 01:01:20,807
If ako veliki deo vašeg razumevanja kako
svet funkcioniše, dolazi sa Facebooka,

1019
01:01:20,851 --> 01:01:23,418
posredstvom algoritamskog softvera

1020
01:01:23,462 --> 01:01:27,118
koji se trudi da vam prikaže
vesti koje biste voleli da vidite,

1021
01:01:27,161 --> 01:01:28,815
to je strašno opasna stvar.

1022
01:01:28,859 --> 01:01:33,080
A ta stvar da mi to još
nismo videli kako se odvija,

1023
01:01:33,124 --> 01:01:37,258
ali smo dozvolili lošim akterima
da dođu do tih informacija...

1024
01:01:37,302 --> 01:01:39,565
Mislim, to je recept za propast.

1025
01:01:43,177 --> 01:01:45,876
Mislim da će se pojaviti
mnogo zlonamernika

1026
01:01:45,919 --> 01:01:48,922
koji će pokušati da AI-jem 
izmanipulišu svet.

1027
01:01:48,966 --> 01:01:52,143
2016. je bila savršen 
primer jednih izbora

1028
01:01:52,186 --> 01:01:55,015
u kojima je učestvovalo dosta AI
koja je pravila mnogo lažnih vesti

1029
01:01:55,059 --> 01:01:58,323
i distribuirala ih sa jasnim
ciljem, kako bi postigla neki rezultat.

1030
01:01:59,890 --> 01:02:02,283
Dame i gospodo,
uvažene kolege...

1031
01:02:02,327 --> 01:02:04,546
velika mi je čast
da danas govorim o

1032
01:02:04,590 --> 01:02:07,985
moći kvantiteta podataka
i psihografikama

1033
01:02:08,028 --> 01:02:09,682
u procesu izbora

1034
01:02:09,726 --> 01:02:12,206
a posebno,
da pričam o radu

1035
01:02:12,250 --> 01:02:14,513
na primarnoj kampanji

1036
01:02:14,556 --> 01:02:16,558
za predsedničke izbore senatoru Cruzu.

1037
01:02:16,602 --> 01:02:19,910
Cambridgeova Analytica
tiho se pojavila kao kompanija

1038
01:02:19,953 --> 01:02:21,563
koja, kako oni sami kažu,

1039
01:02:21,607 --> 01:02:26,307
ima mogućnost da koristi
ovu ogromnu količinu podataka

1040
01:02:26,351 --> 01:02:30,137
sa ciljem da ima uticaj na promenu društva.

1041
01:02:30,181 --> 01:02:33,358
2016. imali su 3 velika klijenta.

1042
01:02:33,401 --> 01:02:34,794
Ted Cruz je bio jedan od njih.

1043
01:02:34,838 --> 01:02:37,884
A lako se zaboravlja,
da je pre samo 18 meseci,

1044
01:02:37,928 --> 01:02:42,846
Senator Cruz bio jedan od
manje popularnih kandidata na izborima.

1045
01:02:42,889 --> 01:02:47,241
Tako da ono što možda nije, kao,
bilo moguće pre 15 ili 10 godina,

1046
01:02:47,285 --> 01:02:49,374
je to da ste mogli da pošaljete
lažnu vest

1047
01:02:49,417 --> 01:02:52,420
tačno onim ljudima kojima
ste želeli da je pošaljete.

1048
01:02:52,464 --> 01:02:56,685
A onda i da zapravo vidite kako
oni reaguju na Facebooku na tu vest

1049
01:02:56,729 --> 01:02:58,905
a onda i da fino podesite
tu informaciju

1050
01:02:58,949 --> 01:03:01,778
prema fidbeku koji ste dobili.

1051
01:03:01,821 --> 01:03:03,257
Kako biste mogli da 
počnete da razvijate

1052
01:03:03,301 --> 01:03:06,130
neku vrstu delovanja na
populaciju uživo.

1053
01:03:06,173 --> 01:03:10,699
U ovom slučaju fokusirali smo se
na grupu koju smo nazvali "Ubeđivanje".

1054
01:03:10,743 --> 01:03:13,746
To su bili ljudi koji
će definitivno glasati,

1055
01:03:13,790 --> 01:03:16,705
za tu partiju, ali kojima
treba malo više

1056
01:03:16,749 --> 01:03:18,490
pomeranja na desnicu.

1057
01:03:18,533 --> 01:03:19,708
kako bi podržali Cruza.

1058
01:03:19,752 --> 01:03:22,059
Njima je trebala ubeđivačka poruka.

1059
01:03:22,102 --> 01:03:23,800
Izabrao sam "Pravo na oružje".

1060
01:03:23,843 --> 01:03:25,802
Koje još malo
sužava to polje.

1061
01:03:25,845 --> 01:03:29,066
I tako sada znamo da nam je potrebna
poruka koja sadrži pravo na oružje,

1062
01:03:29,109 --> 01:03:31,111
a treba da bude
ubeđivačkog karaktera,

1063
01:03:31,155 --> 01:03:34,201
i treba da bude naštelovana
prema određenom tipu ličnosti

1064
01:03:34,245 --> 01:03:36,029
za koji smo mi zainteresovani.

1065
01:03:36,073 --> 01:03:39,946
Putem društvenih mreža, neograničen
broj informacija

1066
01:03:39,990 --> 01:03:42,514
možete da sakupite o
određenoj osobi.

1067
01:03:42,557 --> 01:03:45,734
Raspolažemo sa blizu
4,000 ili 5,000 podataka

1068
01:03:45,778 --> 01:03:48,563
o svakoj odrasloj osobi
u Sjedinjenim državama.

1069
01:03:48,607 --> 01:03:51,915
Ovo se odnosi na
ciljanje na pojedinca.

1070
01:03:51,958 --> 01:03:55,962
Upravo je baš kao i oružje, koje može 
biti upotrebljeno na potpuno pogrešan način.

1071
01:03:56,006 --> 01:03:58,051
To je poblem sa svom
ovom količinom podataka.

1072
01:03:58,095 --> 01:04:02,229
To je skoro kao da smo napravili
metak pre nego što smo napravili pištolj.

1073
01:04:02,273 --> 01:04:06,407
Ted Cruz je implementirao naše podatke,
rezultate naših istraživanja.

1074
01:04:06,451 --> 01:04:09,541
Počeo je sa manje od 5% ljudi

1075
01:04:09,584 --> 01:04:15,590
i imao je spor ali stalan, solidan 
napredak, i tako došao do 35%,

1076
01:04:15,634 --> 01:04:17,157
što ga čini, očigledno,

1077
01:04:17,201 --> 01:04:20,465
drugim najjačim 
rivalom u trci.

1078
01:04:20,508 --> 01:04:23,120
E sada, jasno je da je 
Cruzova kampanja završena,

1079
01:04:23,163 --> 01:04:28,168
ali ono što mogu da vam kažem je to da
od dva kandidata na koja se suzio izbor,

1080
01:04:28,212 --> 01:04:30,867
jedan od njih koristi
ove tehnologije.

1081
01:04:32,564 --> 01:04:35,959
Ja, Donald John Trump,
svečano obećavam

1082
01:04:36,002 --> 01:04:38,222
da ću verno vršiti

1083
01:04:38,265 --> 01:04:42,226
funkciju predsednika
Sjedinjenih država.

1084
01:04:48,275 --> 01:04:50,234
Izbori su bili
marginalna vežba.

1085
01:04:50,277 --> 01:04:53,237
Nije potrebna naročito
sofisticirana AI

1086
01:04:53,280 --> 01:04:57,719
kako biste imali
rezultat tolike disproporcije.

1087
01:04:57,763 --> 01:05:02,550
Još pre Trampa, Brexit je
bio još jedan navodni klijent.

1088
01:05:02,594 --> 01:05:04,726
Ipak, u 20 minuta do 5:00,

1089
01:05:04,770 --> 01:05:08,730
sad možemo reći,
odluka ove zemlje donesena još 1975.

1090
01:05:08,774 --> 01:05:10,950
da se pridruži zajedničkom tržištu

1091
01:05:10,994 --> 01:05:15,999
bila je opozvana ovim referendumom 
da se izađe iz Evropske Unije.

1092
01:05:16,042 --> 01:05:19,828
Cambridge Analytica
je izgleda uz korišćenje AI

1093
01:05:19,872 --> 01:05:23,267
progurala dva najveća bloka

1094
01:05:23,310 --> 01:05:27,967
političkih promena
u poslednjih 50 godina.

1095
01:05:28,011 --> 01:05:30,709
To su epohalni događaji,
a ako verujemo opštem mnjenju,

1096
01:05:30,752 --> 01:05:33,755
oni su direktno povezani
sa delićem softvera,

1097
01:05:33,799 --> 01:05:37,194
kojeg je kreirao
jedan profesor sa Stanforda.

1098
01:05:41,459 --> 01:05:45,593
Još 2013, opisao sam da je
moguće ovo što rade

1099
01:05:45,637 --> 01:05:49,293
i upozorio sam da ovo može
da se ponovi u budućnosti.

1100
01:05:49,336 --> 01:05:51,382
U to vreme, Michal Kosinski

1101
01:05:51,425 --> 01:05:54,994
bio je mladi poljski istraživač
zaposlen u Psychometrics centru.

1102
01:05:55,038 --> 01:06:00,217
Ono na čemu je Michal radio bilo je 
prikupljanje najvećeg ikada seta podataka

1103
01:06:00,260 --> 01:06:03,481
o tome kako se ljudi
ponašaju na Facebook-u.

1104
01:06:03,524 --> 01:06:07,789
Psychometrics radi na
merenju psiholoških osobina,

1105
01:06:07,833 --> 01:06:09,922
kao što su tip ličnosti,
inteligencija,

1106
01:06:09,966 --> 01:06:11,880
politika stremljenja, i tako dalje.

1107
01:06:11,924 --> 01:06:15,058
Tradicija je da se
te osobine mere

1108
01:06:15,101 --> 01:06:17,712
pomoću testova i upitnika.

1109
01:06:17,756 --> 01:06:20,715
Test ličnosti, kao najbenignija
stvar na koju možete pomisliti.

1110
01:06:20,759 --> 01:06:24,197
Nešto što ne mora nužno
imati mnogo primena, zar ne?

1111
01:06:24,241 --> 01:06:27,331
Naša ideja je bila da
umesto testova i upitnika,

1112
01:06:27,374 --> 01:06:30,029
jednostavno možemo posmatrati
digitalne otiske ponašanja

1113
01:06:30,073 --> 01:06:32,553
koje svi ostavljamo za sobom 

1114
01:06:32,597 --> 01:06:34,903
kako bismo razumeli otvorenost,

1115
01:06:34,947 --> 01:06:37,732
svesnost,
neurotičnost.

1116
01:06:37,776 --> 01:06:39,560
Lični podaci
se lako mogu kupiti,

1117
01:06:39,604 --> 01:06:43,129
kao što su oni o vašem prebivalištu,
o klubovima kojima pripadate,

1118
01:06:43,173 --> 01:06:45,044
u koju teretanu idete.

1119
01:06:45,088 --> 01:06:47,873
Postoje zapravo čitava tržišta
za lične podatke.

1120
01:06:47,916 --> 01:06:51,442
Ispada da možemo saznati
neverovatno mnogo o tome šta ćete uraditi

1121
01:06:51,485 --> 01:06:55,750
bazirajući se na vrlo 
malom setu podataka.

1122
01:06:55,794 --> 01:06:58,275
Treniramo
mreže dubokog učenja

1123
01:06:58,318 --> 01:07:01,278
da saznaju intimne osobine,

1124
01:07:01,321 --> 01:07:04,759
političke stavove,
tip ličnosti,

1125
01:07:04,803 --> 01:07:07,806
inteligenciju,
seksualnu orientaciju

1126
01:07:07,849 --> 01:07:10,504
samo imajući sliku
nečijeg lica.

1127
01:07:17,076 --> 01:07:20,645
Pomislite samo na zemlje koje
nisu toliko slobodne i otvorenih shvatanja.

1128
01:07:20,688 --> 01:07:23,300
Ako možete da otkrijete
kojoj veri neko pripada

1129
01:07:23,343 --> 01:07:25,954
ili političko stremljenje
ili seksualnu orijentaciju

1130
01:07:25,998 --> 01:07:28,740
bazirajući se samo na profilnoj slici,

1131
01:07:28,783 --> 01:07:33,310
to može bukvalno biti
pitanje života i smrti.

1132
01:07:37,009 --> 01:07:39,751
Mislim da odavde nema nazad.

1133
01:07:42,145 --> 01:07:44,321
Da li znate šta je Turingov test?

1134
01:07:44,364 --> 01:07:48,977
To je kada je čovek
u interakciji sa kompjuterom,

1135
01:07:49,021 --> 01:07:52,546
i ako čovek ne zna da je to 
kompjuter,

1136
01:07:52,590 --> 01:07:54,126
test je bio položen.

1137
01:07:54,170 --> 01:07:57,247
A tokom sledećih nekoliko dana,

1138
01:07:57,290 --> 01:07:59,684
ti ćeš biti ljudski element
u Turingovom testu.

1139
01:07:59,727 --> 01:08:02,295
Do jaja.
- Tako je, Caleb.

1140
01:08:02,339 --> 01:08:04,080
Shvatio si.

1141
01:08:04,123 --> 01:08:06,865
Jer ako test bude uspešan,

1142
01:08:06,908 --> 01:08:10,825
bićeš ključni deo
najvećeg naučnog događaja

1143
01:08:10,869 --> 01:08:12,958
u istoriji čoveka.

1144
01:08:13,001 --> 01:08:17,615
Ako si ti stvorio svesnu mašinu,
to nije istorija čoveka.

1145
01:08:17,658 --> 01:08:19,356
To je istorija bogova.

1146
01:08:26,841 --> 01:08:29,975
Izgleda da je tehnologija
bog po sebi i za sebe.

1147
01:08:33,196 --> 01:08:35,241
Kao vreme.
Ne možeš na njega uticati.

1148
01:08:35,285 --> 01:08:39,293
Ne možemo ga usporiti.
Ne možemo ga zaustaviti.

1149
01:08:39,637 --> 01:08:42,849
Osećamo se nemoćno.

1150
01:08:43,293 --> 01:08:46,687
Ako boga smatramo za neograničenu
količinu inteligencije,

1151
01:08:46,731 --> 01:08:50,474
najbliže mu možemo prići
evolucijom naše inteligencije

1152
01:08:50,517 --> 01:08:55,566
tako što ćemo je povezati sa
veštačkom inteligencijom koju smo stvorili.

1153
01:08:55,609 --> 01:08:58,003
U ovo vreme, naši kompjuteri, telefoni,

1154
01:08:58,046 --> 01:09:01,615
aplikacije, daju nam
nadljudske sposobnosti.

1155
01:09:01,659 --> 01:09:04,662
Tako da, kao što kaže stara izreka,
Ako ih ne možeš pobediti, pridruži im se.

1156
01:09:06,968 --> 01:09:09,971
Stvar je ljudsko-mašinskog
partnerstva.

1157
01:09:10,015 --> 01:09:11,669
Mislim, već vidim kako, znate,

1158
01:09:11,712 --> 01:09:14,933
na primer, naši telefoni,
služe kao dodatak za mamoriju, zar ne?

1159
01:09:14,976 --> 01:09:17,196
Više ne moram da pamtim
tvoj broj telefona

1160
01:09:17,240 --> 01:09:19,198
jer je u mom telefonu.

1161
01:09:19,242 --> 01:09:22,070
Radi se i o tome da mašine
pojačavaju naše ljudske sposobnosti,

1162
01:09:22,114 --> 01:09:25,248
ne zamenjujući ih potpuno u suprotnom.

1163
01:09:25,249 --> 01:09:27,538
Ako pogledate, sve naprave
koje su načinile prelaz

1164
01:09:27,539 --> 01:09:30,237
u poslednjih 20 godina
sa analognih na digitalne...

1165
01:09:30,238 --> 01:09:32,123
ima ih baš mnogo.

1166
01:09:32,124 --> 01:09:35,388
A mi smo poslednja analogna
naprava u digitalnom univerzumu.

1167
01:09:35,389 --> 01:09:37,068
I, naravno, problem s tim je

1168
01:09:37,069 --> 01:09:40,609
što je ulaz/izlaz podataka
veoma ograničen.

1169
01:09:40,611 --> 01:09:42,613
To je ovo.
A ovo su ovi.

1170
01:09:42,856 --> 01:09:45,355
Oči su nam prilično dobre.

1171
01:09:45,398 --> 01:09:48,445
U stanju smo da primimo
dosta vizuelnih informacija.

1172
01:09:48,488 --> 01:09:52,536
Ali taj izlaz informacija
nam je vrlo, vrlo, vrlo mali.

1173
01:09:52,579 --> 01:09:55,669
Vaoma važan je razlog --
Ako zamislimo scenario

1174
01:09:55,713 --> 01:09:59,543
u kome AI igra jednu značajniju 
ulogu u društvenim sistemima,

1175
01:09:59,586 --> 01:10:02,023
mi želimo na dobar način da
komuniciramo sa tom tehnologijom

1176
01:10:02,067 --> 01:10:04,983
kako bi ona na kraju
poboljšala nas.

1177
01:10:07,855 --> 01:10:12,295
Mislim da je neverovatno važno
da AI ne postane "drugi".

1178
01:10:12,338 --> 01:10:14,562
To moramo biti mi.

1179
01:10:14,906 --> 01:10:18,605
I može biti da ja grešim
u ovome što govorim.

1180
01:10:18,649 --> 01:10:23,915
Svakako sam otvoren za ideje
ako iko može da predloži bolji način.

1181
01:10:23,958 --> 01:10:27,266
Ali zaista mislim da ćemo 
morati ili da se pripojimo VI-ju

1182
01:10:27,310 --> 01:10:29,063
ili da ostanemo van tokova.

1183
01:10:36,406 --> 01:10:38,756
Na neki način teško je
razmišljati o isključivanju sistema

1184
01:10:38,799 --> 01:10:41,802
koji je raširen
svuda po planeti,

1185
01:10:41,846 --> 01:10:45,806
i koji je sad raširen
i po čitavom Sunčevom sistemu.

1186
01:10:45,850 --> 01:10:49,375
Znate kako, ne možete ga sad
jednostavno ugasiti.

1187
01:10:49,419 --> 01:10:51,290
Otvorili smo Pandorinu kutiju.

1188
01:10:51,334 --> 01:10:55,642
Oslobodili smo sile koje ne možemo 
da kontrolišemo, niti da zaustavimo.

1189
01:10:55,686 --> 01:10:59,516
U suštini, mi smo u sred
stvaranja nove vrste života na Zemlji.

1190
01:11:05,870 --> 01:11:07,611
Ne znamo šta će se sledeće dešavati.

1191
01:11:07,654 --> 01:11:10,353
Ne znamo ni kog oblika
će biti inteligencija mašine

1192
01:11:10,396 --> 01:11:14,531
kada je taj intelekt
toliko nedokučiv ljudskom sposobnošću.

1193
01:11:14,574 --> 01:11:17,360
To prosto nije nešto
što je moguće.

1194
01:11:24,758 --> 01:11:26,976
Manje zastrašujuća verzija
budućnosti koju mogu da smislim 

1195
01:11:26,978 --> 01:11:29,633
je ta gde mi imamo
bar malo demokratizovanu AI.

1196
01:11:31,548 --> 01:11:34,159
Jer ako mala kompanija
ili mala grupa ljudi

1197
01:11:34,202 --> 01:11:37,031
uspeva da razvije digitalnu
superinteligenciju nalik bogu,

1198
01:11:37,075 --> 01:11:39,739
oni mogu preuzeti ceo svet.

1199
01:11:40,383 --> 01:11:44,343
Čak kad se pojavi neki zli diktator,
koji je čovek, on će umreti,

1200
01:11:44,387 --> 01:11:46,998
međutim, za AI,
smrt neće biti opcija.

1201
01:11:47,041 --> 01:11:49,392
Ona će živeti zauvek.

1202
01:11:49,435 --> 01:11:53,670
A onda ćemo imati besmrtnog diktatora
od kojeg nikad nećemo moći da pobegnemo.

1203
01:13:33,000 --> 01:13:36,500
Correction and synchronisation:
politfilm.com

1203
01:13:37,305 --> 01:14:37,910
Подржите нас и постаните VIP члан да бисте уклонили све огласе са www.OpenSubtitles.org
